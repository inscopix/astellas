{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "import bisect\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import transform\n",
    "\n",
    "from scipy import stats\n",
    "import scipy.signal as signal\n",
    "import scipy.interpolate as interpolate\n",
    "from scipy import ndimage as nd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from skimage import transform\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import isx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_background_images(in_movie, seg_times, n_background_frames = 500, fps = 20):\n",
    "    #\n",
    "    # \n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    print('Background detection assuming {}fps video\\n'.format(fps))\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    vid = cv2.VideoCapture(in_movie)\n",
    "    num_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    vid_size = [int(vid.get(4)), int(vid.get(3))]\n",
    "    \n",
    "    # convert segments to indices and add start/stop frames:\n",
    "    seg_times = [int(i*fps) for i in seg_times]\n",
    "    seg_times.append(num_frames)\n",
    "    seg_times.insert(0, 0)\n",
    "    seg_idx = [[i, j] for i,j in zip(seg_times[:-1], seg_times[1:]) ]\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    # build background image for each segment:\n",
    "    for seg in tqdm(seg_idx):\n",
    "        framevect = np.unique(rng.integers(seg[0], seg[1], int(n_background_frames * 2)))[:n_background_frames] # draw n_background_frames without replacement\n",
    "        background_dat = np.zeros( (len(framevect), vid_size[0], vid_size[1]) ).astype('uint8')\n",
    "        #vid = cv2.VideoCapture(vid_fn)\n",
    "        #bidx = 0\n",
    "        for bidx,i in enumerate(framevect):\n",
    "            vid.set(1, i)\n",
    "            theframe = vid.read()[1]\n",
    "            grayscale = cv2.cvtColor(theframe,cv2.COLOR_BGR2GRAY).astype('uint8')\n",
    "            #grayscale = (np.sum(theframe,axis=2)/3).astype('uint8')\n",
    "            background_dat[bidx] = grayscale\n",
    "            #bidx += 1        \n",
    "\n",
    "        #background_frame = np.median(background_dat, axis=0)\n",
    "        out_list.append(np.median(background_dat, axis=0))\n",
    "        \n",
    "\n",
    "    \n",
    "    vid.release()\n",
    "    return out_list\n",
    "\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "\n",
    "def movie_com(in_movie, background_movie, seg_times, filt_size = 10, ds_fact = 4, pthresh = 99, fps = 20):\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "\n",
    "    vid = cv2.VideoCapture(in_movie)\n",
    "    num_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_range = list(range(num_frames))\n",
    "\n",
    "    # initialize output:\n",
    "    com_vect = np.zeros((len(frame_range),2)).astype('int32')\n",
    "    \n",
    "    seg_times = [int(i*fps) for i in seg_times]\n",
    "    seg_times.append(num_frames)\n",
    "    seg_times.insert(0, 0)\n",
    "    seg_idx = [[i, j] for i,j in zip(seg_times[:-1], seg_times[1:]) ]\n",
    "    \n",
    "    movie_epochs = [np.arange(i[0],i[1]) for i in seg_idx]\n",
    "    print(np.shape(movie_epochs))\n",
    "\n",
    "    for framei in tqdm(range(len(frame_range))):\n",
    "        vid.set(1, framei)\n",
    "        theframe = vid.read()[1]\n",
    "\n",
    "        background_frame_ds = transform.downscale_local_mean(background_movie[np.argwhere([framei in i for i in movie_epochs])[0][0]], (ds_fact, ds_fact))\n",
    "        grayscale = transform.downscale_local_mean( cv2.cvtColor(theframe,cv2.COLOR_BGR2GRAY).astype('uint8'), (ds_fact,ds_fact))\n",
    "\n",
    "        bg_subtracted = grayscale - background_frame_ds\n",
    "        bg_subtracted[bg_subtracted > np.percentile(bg_subtracted, 100-pthresh)] = 0\n",
    "        bg_subtracted[bg_subtracted > 0] = 255\n",
    "\n",
    "        center_of_mass = nd.center_of_mass(nd.maximum_filter(bg_subtracted, int(filt_size / ds_fact )))\n",
    "        \n",
    "        if sum(np.isnan(center_of_mass)):\n",
    "            com_vect[framei] = com_vect[framei -1]\n",
    "        else:\n",
    "            com_vect[framei] = [i for i in center_of_mass]\n",
    "\n",
    "    vid.release()\n",
    "    return com_vect\n",
    "\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butterworth(sig,cutoff,fs,filt_order,filt_type):\n",
    "    #\n",
    "    # applies zero-phase digital butterworth filter to data in sig\n",
    "    # if sig is a matrix, data in its columns are filtered\n",
    "    #\n",
    "    # cutoff is filter's critical frequency in Hz, or n=2 tuple of freqs for bandpass filters\n",
    "    # fs is sampling rate\n",
    "    # filt_order is integer filter order\n",
    "    # filt_type is 'lowpass','highpass', 'bandpass', or 'bandstop' \n",
    "    # \n",
    "    # returns filtered signal \n",
    "    #\n",
    "    \n",
    "    nyq = 0.5 * fs\n",
    "    norm_cutoff =cutoff/nyq\n",
    "    b,a = signal.butter(filt_order,norm_cutoff,btype=filt_type,analog=False)\n",
    "    \n",
    "    if sig.ndim > 1:\n",
    "        filt_sig = signal.filtfilt(b,a,sig,axis=1)\n",
    "    elif sig.ndim == 1:\n",
    "        filt_sig = signal.filtfilt(b,a,sig)\n",
    "    else:\n",
    "        filt_sig = []\n",
    "    \n",
    "    return filt_sig\n",
    "\n",
    "def median_filter_matrix(inmtx,winsize=3):\n",
    "    #\n",
    "    # median filters each column of matrix with kernal width = winsize\n",
    "    #\n",
    "    # returns filtered matrix\n",
    "    #\n",
    "    \n",
    "    filtmtx = np.zeros_like(inmtx)\n",
    "    \n",
    "    for i in range(inmtx.shape[0]):\n",
    "        filtmtx[i] = signal.medfilt(inmtx[i],winsize)\n",
    "    return filtmtx\n",
    "\n",
    "def otsu(x):\n",
    "    # returns threshold estimated via Otsu's method\n",
    "    # x needs to be 1-d list\n",
    "    nbins = 500\n",
    "    logx = np.log(x)\n",
    "    logx = np.asarray([i for i in logx if np.isfinite(i)])\n",
    "    \n",
    "    minim = min(logx)\n",
    "    maxim = max(logx)\n",
    "    logx = (logx-minim)/(maxim-minim)\n",
    "    \n",
    "    xhist = np.histogram(logx, bins=nbins)\n",
    "    xbins = xhist[1]\n",
    "    xbins = xbins[1:]\n",
    "    xcounts = xhist[0] / sum(xhist[0])\n",
    "\n",
    "    maximum = 0\n",
    "    thresh = 0\n",
    "    \n",
    "    for t in range(len(xcounts)):\n",
    "        w0 = sum(xcounts[:t])\n",
    "        w1 = sum(xcounts) - w0\n",
    "        if w0 == 0 or w1 == 0:\n",
    "            #print('continuing...')\n",
    "            continue\n",
    "        mu0 = sum(xbins[:t]*xcounts[:t]) / w0\n",
    "        mu1 = sum(xbins[t:]*xcounts[t:]) / w1\n",
    "        sigB = w0 * w1 * ((mu0 - mu1) * (mu0 - mu1))\n",
    "        if sigB >= maximum:\n",
    "            maximum = sigB\n",
    "            thresh = xbins[t]\n",
    "    \n",
    "    th = thresh*(maxim-minim) + minim\n",
    "    th = np.exp(th)\n",
    "    #th = np.power(10,th)\n",
    "    thresh = th\n",
    "\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def register_signals(ref_time,in_dat,in_time):\n",
    "    #\n",
    "    # Direct alignment of signals that were sampled at diffrent rates\n",
    "    # resamples in_dat to len(ref_time) by aligning timestamps in ref_time with timestamps in in_time\n",
    "    # in_time is vector of timestamps for data in in_dat\n",
    "    #\n",
    "    # len(ref_time) < len(in_time)\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    resamp_dat = np.zeros_like(ref_time)\n",
    "\n",
    "    for t in range(len(ref_time)):\n",
    "        dat = in_dat[bisect.bisect(in_time,ref_time[t])-1]\n",
    "        resamp_dat[t] = dat\n",
    "    return resamp_dat\n",
    "    \n",
    "def segment(x,thresh):\n",
    "    # returns onsets & offset indices of impulses that exceed thresh in vector x\n",
    "    xsub = x - thresh\n",
    "    xdiff = (xsub[:-1]*xsub[1:]) < 0\n",
    "    edges = np.where(xdiff == 1) # indices at onsets/offsets\n",
    "    edges = list(edges[0])\n",
    "    \n",
    "    x[0:5] = 0\n",
    "    \n",
    "    if np.mean(np.diff(x[edges[0]-2:edges[0]+2])) < 0: # delete first impulse if offset\n",
    "        print('first impulse is offset')\n",
    "        edges.pop(0)\n",
    "    if np.mean(np.diff(x[edges[-1]-2:edges[-1]+2])) > 0: # delete last impulse if onset\n",
    "        print('last impulse is onset')\n",
    "        edges.pop(-1)\n",
    "    onsets = edges[0::2]\n",
    "    offsets = edges[1::2]\n",
    "    \n",
    "    if len(onsets) > len(offsets):\n",
    "        onsets.pop(-1)\n",
    "    \n",
    "    return [onsets,offsets]\n",
    "\n",
    "def extractsegments(x,onsets,offsets,win):\n",
    "    # returns list of slices from x[onsets[i]-win:offsets[i]+win]\n",
    "    \n",
    "    win = int(win)\n",
    "    if onsets[0] - win < 1 or offsets[-1] > len(x):\n",
    "        print('Edge Syllable')\n",
    "        return []\n",
    "    else:\n",
    "        segs = [x[onsets[i]-win : offsets[i]+win] for i in range(len(onsets))]\n",
    "        return segs\n",
    "    \n",
    "def filtersegments(segs,minlen,maxlen):\n",
    "    # deletes elements of segs with maxlen < len() < minlen\n",
    "    return [i for i in segs if len(i) > minlen and len(i) < maxlen]\n",
    "    \n",
    "def twoaxis(axname,lwidth=2):\n",
    "    #\n",
    "    # formats axes by setting axis thickness & ticks to lwidth and clearing top/right axes\n",
    "    #\n",
    "    #\n",
    "    axname.spines['bottom'].set_linewidth(lwidth)\n",
    "    axname.tick_params(width=lwidth)\n",
    "    axname.spines['left'].set_linewidth(lwidth)\n",
    "    axname.spines['top'].set_linewidth(0)\n",
    "    axname.spines['right'].set_linewidth(0)\n",
    "    \n",
    "# functions for object detection:\n",
    "\n",
    "def subtract_img_background(input_img):\n",
    "    #\n",
    "    # Uses pixel dilation to adaptively filter nonuniform background from objects in input_img\n",
    "    #\n",
    "    # input image is NxM  matrix of pixel intensities \n",
    "    #\n",
    "    # returns filtered image.\n",
    "    #\n",
    "    # Adapted from skimage 'Filtering regional maxima'. Requires : \n",
    "    # import numpy as np\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # from scipy.ndimage import gaussian_filter\n",
    "    # from skimage.filters import threshold_otsu\n",
    "    # #from skimage import img_as_float\n",
    "    # from skimage.morphology import reconstruction\n",
    "    #\n",
    "    #\n",
    "    img = img_as_float(input_img)\n",
    "    img_g = gaussian_filter(img,1)\n",
    "\n",
    "    h = threshold_otsu(img_g)\n",
    "\n",
    "    seed = img_g - h\n",
    "    mask = img_g\n",
    "\n",
    "    dilated = reconstruction(seed,mask,method='dilation')\n",
    "\n",
    "    return img_g-dilated\n",
    "\n",
    "def norm_image(in_img):\n",
    "    #\n",
    "    # returns in_img normalized to [0-255]\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    i_min = np.min(in_img)\n",
    "    i_max = np.max(in_img)\n",
    "    \n",
    "    norm_img = 255 * ((in_img + abs(i_min) / i_max))\n",
    "    \n",
    "    return norm_img\n",
    "\n",
    "def estimate_num_cells(in_img):\n",
    "    #\n",
    "    # Estimates number of gaussian-like blobs in in_img using laplacian of gaussians (LoG)\n",
    "    #\n",
    "    # \n",
    "    # in_img is NxM matrix of pixel intensities\n",
    "    # in_img is probably maximum or other projection of motion corrected or df/f movie\n",
    "    #\n",
    "    # requires skimage.feature.blob_log()\n",
    "    #\n",
    "    # returns N x 3 matrix, N=num_blobs, out[0] = y coordinates, out[1] = x coordinates, out[2] = blob radius \n",
    "    #\n",
    "    \n",
    "    # log_params:\n",
    "    min_sigma = 2\n",
    "    max_sigma = 20\n",
    "    num_sigma = int((max_sigma-min_sigma))\n",
    "    threshold = 1\n",
    "    overlap =0.1\n",
    "    \n",
    "    img_background_subtract = subtract_img_background(in_img)\n",
    "    img_norm = norm_image(img_background_subtract)\n",
    "    objs = blob_log(img_norm,min_sigma = min_sigma,max_sigma=max_sigma,num_sigma=num_sigma,threshold=threshold,overlap=overlap)\n",
    "    \n",
    "    return objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nans(indat):\n",
    "    #\n",
    "    # returns the number of NaN values in indat\n",
    "    # \n",
    "    \n",
    "    return np.isnan(indat).sum()\n",
    "\n",
    "def events_to_raster(event_times,timing,smooth_win=0):\n",
    "    #\n",
    "    # returns raster and length vector of event_times, optionally smoothed with gaussian of smooth_win number of pnts\n",
    "    #\n",
    "    # timing is <class 'isx.core.Timing'> from EventSet\n",
    "    #\n",
    "    # Example usage: offs,amps = events_1.get_cell_data(0)\n",
    "    # [rast,tvect] = events_to_raster(offs,events_1.timing,0)\n",
    "    # plot tvect vs rast\n",
    "    #\n",
    "    \n",
    "    samp_vect = [i for i in range(timing.num_samples)]\n",
    "    event_times_samps = event_times / timing.period.to_usecs()\n",
    "    raster = np.zeros_like(samp_vect)\n",
    "    if len(event_times) == 0:\n",
    "        raster = [np.nan for i in raster]        \n",
    "        return[raster,samp_vect]\n",
    "    else:    \n",
    "        raster[event_times_samps.astype(int)] = 1\n",
    "    \n",
    "        if smooth_win > 0:\n",
    "            win = signal.gaussian(int(smooth_win)*10,smooth_win)\n",
    "            raster_sm = np.convolve(raster,win,mode='same')\n",
    "            raster = raster_sm\n",
    "        \n",
    "        return [raster,samp_vect]\n",
    "\n",
    "def rasters_from_eventset(eventset,sm_win=0):\n",
    "    #\n",
    "    # returns numcell X sample matrix of rasters derived from eventset\n",
    "    #\n",
    "    # optionally smooths rasters with gaussian window with std deviation = sm_win samples\n",
    "    #\n",
    "    \n",
    "    offs,amps = eventset.get_cell_data(0)\n",
    "    raster,raster_x = events_to_raster(offs,eventset.timing,sm_win)\n",
    "    outmtx = np.zeros([len(raster),eventset.num_cells])\n",
    "    for cellnum in range(eventset.num_cells):\n",
    "        offs,amps = eventset.get_cell_data(cellnum)\n",
    "        raster,raster_x = events_to_raster(offs,eventset.timing,sm_win)\n",
    "        outmtx[:,cellnum] = raster\n",
    "    \n",
    "    return outmtx\n",
    " \n",
    "def traces_from_cellset(cellset,nrm=0):\n",
    "    #\n",
    "    # returns numcells x max(tstamps) matrix of traces from cellset\n",
    "    # centers (mean subtracts) each trace if nrm==1\n",
    "    #\n",
    "    \n",
    "    tracelen = 0\n",
    "    for i in range(cellset.num_cells): # find length of traces for initialization:\n",
    "        if len(cellset.get_cell_trace_data(0)) > tracelen:\n",
    "            tracelen = len(cellset.get_cell_trace_data(0))\n",
    "            break\n",
    "    outmtx = np.zeros([tracelen,cellset.num_cells]) # init output\n",
    "    for i in range(cellset.num_cells): # populate output with traces from cellsets:\n",
    "        if nrm == 1 and sum(abs(cellset.get_cell_trace_data(i)))>0:\n",
    "            outmtx[:,i] = cellset.get_cell_trace_data(i) - np.nanmean(cellset.get_cell_trace_data(i))\n",
    "        else:    \n",
    "            outmtx[:,i] = cellset.get_cell_trace_data(i)\n",
    "    return outmtx\n",
    "\n",
    "def images_from_cellset(cellset):\n",
    "    #\n",
    "    # returns numcells x max(tstamps) matrix of images from cellset\n",
    "    #\n",
    "    img = cellset.get_cell_image_data(0)\n",
    "    \n",
    "    outmtx = np.zeros([img.shape[0],img.shape[1],cellset.num_cells]) # init output\n",
    "    for i in range(cellset.num_cells): # populate output with traces from cellsets:\n",
    "        outmtx[:,:,i] = cellset.get_cell_image_data(i)\n",
    "    return outmtx\n",
    "\n",
    "\n",
    "def remove_nan_rows(inmtx_list):\n",
    "    #\n",
    "    # Detects rows with missing data points (NaNs) in each matrix in inmtx_list. Removes rows from all elements\n",
    "    # of inmtx_list\n",
    "    #\n",
    "    \n",
    "    nanrows = []\n",
    "    outlist = []\n",
    "    for mtx in inmtx_list:\n",
    "        livecols = np.where(np.all(np.isnan(mtx),axis=0)==False)[0] # find columns that contain data\n",
    "        thenanrows = np.where(np.any(np.isnan(mtx[:,livecols]),axis=1))[0] # find rows with NaNs\n",
    "        nanrows.append(thenanrows)\n",
    "    del_these_rows = [i for lst in nanrows for i in lst] # flattens lists in nanrows\n",
    "    \n",
    "    for mtx in inmtx_list:\n",
    "        outmtx = np.delete(mtx,del_these_rows,axis=0)\n",
    "        #outlist.append(np.delete(mtx,del_these_rows,axis=0))\n",
    "        outlist.append(outmtx)\n",
    "    \n",
    "    return outlist\n",
    "\n",
    "def build_trace_volume(cellset_list):\n",
    "    #\n",
    "    # assembles [p,l,c] matrix of traces from list of LR cellsets. p is number of planes/cellsets, l is data\n",
    "    # length, c is number of cells.\n",
    "    #\n",
    "    # removes dropped/cropped frames from all cellsets' data\n",
    "    # removes additional rows with missing data from all cellsets' data\n",
    "    #\n",
    "    # if traces are not equal length, crops ends to minimum length\n",
    "    #\n",
    "    # returns trace_matrix\n",
    "    # \n",
    "    \n",
    "    remove_frames = np.array([],dtype=int)\n",
    "    tracemtx_list = []\n",
    "    for l in cellset_list:\n",
    "        cellset = isx.CellSet.read(l)\n",
    "        remove_frames = np.append(remove_frames,cellset.timing.dropped)\n",
    "        remove_frames = np.append(remove_frames,cellset.timing.cropped)\n",
    "        tracemtx_list.append(traces_from_cellset(cellset,nrm=1))\n",
    "    \n",
    "    clean_list = []\n",
    "    for m in tracemtx_list:\n",
    "        outmtx = np.delete(m,remove_frames.flatten().astype(int),axis=0)\n",
    "        clean_list.append(outmtx)    \n",
    "    \n",
    "    clean_list_nonan = remove_nan_rows(clean_list)\n",
    "    npnts = min([len(i) for i in clean_list_nonan])\n",
    "    out_list = [i[0:npnts,:] for i in clean_list_nonan]\n",
    "    trace_vol = np.stack(out_list,axis=0)\n",
    "    \n",
    "    return trace_vol\n",
    "\n",
    "def build_img_volume(cellset_list):\n",
    "    #\n",
    "    # assembles [p,x,y,c] matrix of images from list of LR cellsets. p is number of planes/cellsets, l is data\n",
    "    # length, c is number of cells.\n",
    "    #\n",
    "    # returns image matrix\n",
    "    #\n",
    "\n",
    "    mtx_list = []\n",
    "    for l in cellset_list:\n",
    "        cellset = isx.CellSet.read(l)\n",
    "        mtx_list.append(images_from_cellset(cellset))\n",
    "    \n",
    "    img_vol = np.stack(mtx_list,axis=0)\n",
    "    \n",
    "    return img_vol\n",
    "\n",
    "def build_raster_volume(eventset_list,sm_win=0):\n",
    "    #\n",
    "    # assembles [p,x,c] matrix of rasters from list of LR cellsets. p is number of planes/cellsets, x is data\n",
    "    # length, c is number of cells.\n",
    "    #\n",
    "    # optionally smooths rasters with gaussian with std = sm_win samples \n",
    "    #\n",
    "    # returns raster matrix\n",
    "    #\n",
    "\n",
    "    mtx_list = []\n",
    "    for l in eventset_list:\n",
    "        eventset = isx.EventSet.read(l)\n",
    "        mtx_list.append(rasters_from_eventset(eventset,sm_win))\n",
    "    \n",
    "    rast_vol = np.stack(mtx_list,axis=0)\n",
    "    \n",
    "    return rast_vol\n",
    "    \n",
    "def find_LR_planes(inmtx):\n",
    "    #\n",
    "    # detects plane boundaries in Longitudinally registered matrix \n",
    "    #\n",
    "    # returns list of start and stop points for each plane, e.g for 3 planes: [[0:100],[101:200],[201:300]]\n",
    "    #\n",
    "    \n",
    "    outpnts = []\n",
    "    start_p = 0\n",
    "    for p in range(inmtx.shape[0]): # each plane\n",
    "        if p == inmtx.shape[0]-1:\n",
    "            stop_p = inmtx.shape[2]\n",
    "        #elif len(np.nonzero(np.isnan(inmtx[p,0,start_p:]))):\n",
    "        elif np.shape(np.nonzero(np.isnan(inmtx[p,0,start_p:])))[1]:\n",
    "            print(p)\n",
    "            #print(np.isnan(inmtx[p,0,start_p:]))\n",
    "            print(np.shape(np.nonzero(np.isnan(inmtx[p,0,start_p:]))))\n",
    "            print(np.shape(np.nonzero(np.isnan(inmtx[p,0,start_p:])))[1])\n",
    "            stop_p = np.min(np.nonzero(np.isnan(inmtx[p,0,start_p:])))+start_p\n",
    "        else:\n",
    "            stop_p = inmtx.shape[2]\n",
    "        outpnts.append([start_p,stop_p-1])\n",
    "        start_p = stop_p\n",
    "        \n",
    "    return outpnts\n",
    "\n",
    "def corr_dist_1d_volume(inmtx,bkpnts):\n",
    "#\n",
    "# returns distrubution of trace x trace correlations for signals within a plane\n",
    "#\n",
    "# inmtx is matrix of union cellsets with dimensions [num_planes,data_length,num_signals]\n",
    "# bkpnts is array of endpoints that separate imaging planes, e.g. if bkpnts = [100,200,500],\n",
    "# units IDed on plane 1 are inmtx[0,:,0:100], units IDed on plane 2 are inmtx[1,:,101:200], etc\n",
    "# Bkpnts should be same length of np.shape(inmtx)[0]\n",
    "#\n",
    "# to call this function on first plane in matrix, use corr_dist_1d_volume(mtx[0:1,:,:])\n",
    "# to call this function on second plane in matrix, use corr_dist_1d_volume(mtx[1:2,:,:]) etc\n",
    "#\n",
    "    corrlist = []\n",
    "    \n",
    "    pnt_i = 0\n",
    "    startpnt = 0\n",
    "    for plane in inmtx:\n",
    "        subplane = plane[:,startpnt:bkpnts[pnt_i]]\n",
    "        #print(np.shape(subplane),end='')\n",
    "        corrmtx = (np.corrcoef(subplane,rowvar=False))\n",
    "        corrs = corrmtx[np.triu_indices(corrmtx.shape[0],k=1)]\n",
    "        corrlist.append(corrs)\n",
    "        \n",
    "        startpnt = bkpnts[pnt_i]+1\n",
    "        pnt_i += 1\n",
    "        \n",
    "    #return np.concatenate(corrlist)\n",
    "    return corrlist\n",
    "\n",
    "def corr_dist_z_volume(inmtx,bkpnts):\n",
    "#\n",
    "# returns list of distrubutions of trace x trace correlations for distinct xy signals between planes.\n",
    "# also returns list of indices of planes analyzed to serve as a lookup table\n",
    "#\n",
    "# inmtx is matrix of union cellsets with dimensions [num_planes,data_length,num_signals]\n",
    "# bkpnts is array of start-endpoints that separate imaging planes, e.g. if bkpnts = [[0,100],[101,200],[201,500]],\n",
    "# units IDed on plane 1 are inmtx[0,:,0:100], units IDed on plane 2 are inmtx[1,:,101:200], etc\n",
    "# Bkpnts should be same length of np.shape(inmtx)[0]\n",
    "#\n",
    "\n",
    "    corrlist = []\n",
    "    corrmtxlist = []\n",
    "    \n",
    "    plane_list = list(range(np.shape(inmtx)[0]))\n",
    "    plane_combos = list(itl.combinations(plane_list,2))\n",
    "    \n",
    "    for f in plane_combos:\n",
    "        subplane1 = inmtx[f[0],:,bkpnts[f[0]][0]:bkpnts[f[0]][1]]\n",
    "        subplane2 = inmtx[f[1],:,bkpnts[f[1]][0]:bkpnts[f[1]][1]]\n",
    "        mergeplane = np.concatenate([subplane1,subplane2],axis=1)\n",
    "        \n",
    "        corrmtx = (np.corrcoef(mergeplane,rowvar=False))        \n",
    "        corrs = corrmtx[0:bkpnts[f[0]][1]-bkpnts[f[0]][0],bkpnts[f[0]][1]-bkpnts[f[0]][0]+1:len(corrmtx)]\n",
    "        corrmtxlist.append(corrs)\n",
    "        corrlist.append(np.ravel(corrs))\n",
    "        \n",
    "    return [corrlist,corrmtxlist,plane_combos]\n",
    "\n",
    "def aligned_z_corrs(data_volume,threshs = []):\n",
    "    #\n",
    "    # returns distribution of z-correlations in data_volume\n",
    "    # Optionally makes retain/reject decision for each element based on threshs\n",
    "    #\n",
    "    \n",
    "    plane_list = list(range(np.shape(data_volume)[0]))\n",
    "    plane_combos = list(itl.combinations(plane_list,2))\n",
    "    \n",
    "    datacorrs = []\n",
    "    \n",
    "    if len(threshs) > 0:\n",
    "        decision_array = np.zeros([trace_volume.shape[2],1])\n",
    "        decision_planes = [[] for i in decision_array]\n",
    "        splitcorrs = []\n",
    "    \n",
    "    for i in range(data_volume.shape[2]): # each cell\n",
    "        combo_i = 0\n",
    "        for f in plane_combos: # each pairwise plane set\n",
    "            #corrthresh = corr_threshs[f[0],f[1]]\n",
    "            if len(threshs) > 0:\n",
    "                corrthresh = corr_threshs[combo_i]\n",
    "                combo_i += 1\n",
    "                #print(corrthresh)\n",
    "            trace1 = data_volume[f[0],:,i]\n",
    "            trace2 = data_volume[f[1],:,i]\n",
    "        \n",
    "            if np.isnan(trace1).sum() == 0 and np.isnan(trace2).sum() == 0: # if cell has signal on multiple planes then split/merge:\n",
    "                thecorr = np.corrcoef(trace1,trace2)[0][1]\n",
    "                datacorrs.append(thecorr)\n",
    "                if len(threshs) > 0:\n",
    "                    if thecorr < corrthresh: # split cells\n",
    "                        decision_array[i] = 1\n",
    "                        decision_planes[i].append(f)\n",
    "                        splitcorrs.append(round(thecorr,2))\n",
    "\n",
    "    if len(threshs) > 0:\n",
    "        return [datacorrs,decision_array,decision_planes,splitcorrs]\n",
    "    else:\n",
    "        return datacorrs\n",
    "    \n",
    "def write_1d_cellset(fn,trace_data,img_data,cell_names,spacing,timing):\n",
    "    #\n",
    "    # writes .isxd cellset file to filename = fn\n",
    "    # trace_data is [n,c] shaped matrix of activity traces. n = trace length, c = number of cells\n",
    "    # img_data is [x,y,c] shaped matrix of cell images. x and y = image dimensions, c = number of cells\n",
    "    #\n",
    "    # cell_names is list of strings to name each cell. If this is empty, cells are named sequentially.\n",
    "    # \n",
    "    # spacing and timing are given by data movies and describe image size and activity (trace) sampling info\n",
    "    #\n",
    "    # overwrites existing cellsets with same names without warning\n",
    "    #\n",
    "    \n",
    "    if os.path.exists(fn) is False:\n",
    "        out_cellset = isx.CellSet.write(fn,timing,spacing)\n",
    "    elif os.path.exists(fn) is True:\n",
    "        os.remove(fn)\n",
    "        out_cellset = isx.CellSet.write(fn,timing,spacing)\n",
    "        \n",
    "    for i in range(trace_data.shape[1]): # each cell        \n",
    "        if len(cell_names) == 0:\n",
    "            c_name = 'C{}'.format(i)\n",
    "            \n",
    "        else:\n",
    "            c_name = cell_names[i]\n",
    "        out_cellset.set_cell_data(i,img_data[:,:,i].astype(np.float32),trace_data[:,i].astype(np.float32),c_name)\n",
    "    \n",
    "    out_cellset.flush()\n",
    "    \n",
    "def sensitivity_precision(data_volume,winsize = 5):\n",
    "    #\n",
    "    # returns sensitivity and precision calcs for each element n [:,:,n] in data_volume\n",
    "    #\n",
    "    # data_volume[,n,:] are event traces over time (e.g. event trains / rasters)\n",
    "    # data_volume[n,:,:] are reference and test sets\n",
    "    #\n",
    "    # winsize is integer size of window around events\n",
    "    #\n",
    "    \n",
    "    conv_vect = np.ones((1,winsize))[0]\n",
    "    \n",
    "    sens = []\n",
    "    prec = []\n",
    "    \n",
    "    for i in range(data_volume.shape[2]): # each element [:,:,i]\n",
    "        ref_trace = data_volume[0,:,i]\n",
    "        test_trace = data_volume[1,:,i]\n",
    "        if np.isnan(ref_trace).sum() == 0 and np.isnan(test_trace).sum() == 0: # if cell has signal on both planes:\n",
    "            # detected (test) events:\n",
    "            sm_rast_det = np.convolve(test_trace,conv_vect,mode='same')\n",
    "            sm_rast_det[sm_rast_det > 0] = 1\n",
    "            det_pos_pnts = np.nonzero(sm_rast_det)[0] # points around test events\n",
    "            det_neg = [i for i in range(len(ref_trace)) if i not in det_pos_pnts] # points around test negatives\n",
    "            \n",
    "            true_pos = (sum(ref_trace[det_pos_pnts]))\n",
    "            false_pos = max([sum(test_trace) - true_pos, 0])\n",
    "            false_neg = sum(ref_trace[det_neg])\n",
    "            #true_neg = len([i for i in ref_neg if i in det_neg])\n",
    "            \n",
    "            prec.append(true_pos / (true_pos + false_pos)) # precision: fraction of correctly detected events among all detected events\n",
    "            sens.append(true_pos / (true_pos + false_neg)) # sensitivity: fraction of events detected\n",
    "            \n",
    "\n",
    "    return [sens,prec] \n",
    "    \n",
    "def event_detection_cnmfe(cellset, snr_thresh = 1, peak_dist = 0.25, peak_width = 0.4):\n",
    "    #\n",
    "    # Applies scipy.signal.findpeaks() to identify events in CNMF-E or PCA-ICA cellset traces and writes isx.Eventset\n",
    "    #\n",
    "    # Designed to detect events in cnmfe cellsets but also applicable to PCA-ICA cellsets\n",
    "    #\n",
    "    # Computes threshold for each trace as [snr_thresh * (abs(median_abs_deviation / median) * median_abs_deviation)] \n",
    "    #\n",
    "    # \n",
    "    #\n",
    "    \n",
    "    ed_file = isx.make_output_file_path(cellset.file_path, os.path.dirname(cellset.file_path), 'ED')\n",
    "    if os.path.isfile(ed_file):\n",
    "        os.remove(ed_file)\n",
    "    cellnames = [cellset.get_cell_name(i) for i in range(cellset.num_cells)]    \n",
    "    ed_set = isx.EventSet.write(ed_file, cellset.timing, cellnames)\n",
    "    \n",
    "    cellset_fs = round((1 / (cellset.timing.period.to_usecs() * 1e-6) ),3)\n",
    "    #print(cellset_fs)\n",
    "    pdist = int(peak_dist * cellset_fs)\n",
    "    pwidth = int(peak_width * cellset_fs)\n",
    "    \n",
    "    for i in range(cellset.num_cells):\n",
    "        the_trace = cellset.get_cell_trace_data(i)\n",
    "\n",
    "        thresh_mult = snr_thresh * abs(stats.median_absolute_deviation(the_trace) / np.median(the_trace))\n",
    "        peak_thresh = thresh_mult * abs(stats.median_absolute_deviation(the_trace))        \n",
    "        \n",
    "        peaks = signal.find_peaks(the_trace, peak_thresh, distance= pdist, width= pwidth)[0]\n",
    "        ed_set.set_cell_data(i, 1e6*(peaks/cellset_fs), the_trace[peaks])\n",
    "    ed_set.flush()\n",
    "    return ed_set.file_path\n",
    "    \n",
    "    \n",
    "def cellset_qc(cellset_fn, eventset_fn, filters):\n",
    "    #\n",
    "    # applies conditions to accept/reject cells in cellset based on eventset\n",
    "    #\n",
    "    # filters format = [('SNR', '>', 5), ('Event Rate', '>', 0.0015), ('# Comps', '=', 1)]\n",
    "    #\n",
    "    \n",
    "    cellset = isx.CellSet.read(cellset_fn,read_only=False)\n",
    "    if os.path.isfile(eventset_fn):\n",
    "        eventset = isx.EventSet.read(eventset_fn)\n",
    "    else: \n",
    "        event_detection_cnmfe(cellset)\n",
    "        eventset = isx.EventSet.read(eventset_fn)\n",
    "    metric_fn = isx.make_output_file_path(cellset_fn, os.path.dirname(cellset_fn), 'METRICS')\n",
    "    if os.path.isfile(metric_fn):\n",
    "        os.remove(metric_fn)\n",
    "    isx.cell_metrics(cellset_fn, eventset_fn, metric_fn)\n",
    "    metrics_df = pd.read_csv(metric_fn)\n",
    "    metrics_df.columns = [i.lower() for i in metrics_df.columns]\n",
    "    \n",
    "    cell_rej = []\n",
    "    tmp_rej = []\n",
    "    for i in filters:\n",
    "        if i[0].lower() in metrics_df.columns: # identify cells that fail criterion:\n",
    "            tmp_rej = []\n",
    "            tmp_rej.append(metrics_df.loc[metrics_df[i[0].lower()] <= float(i[2])].index)\n",
    "            #print('Checking cells {}...'.format(i[0]))\n",
    "            cell_rej += tmp_rej\n",
    "        else: \n",
    "            print('{} is not a valid metric.'.format(i[0]))\n",
    "\n",
    "    os.remove(metric_fn)\n",
    "    \n",
    "    out_list = sorted(list(set([i for j in cell_rej for i in j])))\n",
    "    accept_list = [i for i in range(cellset.num_cells) if i not in out_list]\n",
    "    \n",
    "    # modify cellsets:\n",
    "    for i in out_list:\n",
    "        cellset.set_cell_status(i,'rejected')\n",
    "    for i in accept_list:\n",
    "        cellset.set_cell_status(i,'accepted')    \n",
    "    \n",
    "    \n",
    "    return out_list\n",
    "    \n",
    "    \n",
    "def twoaxis(axname,lwidth=1.5):\n",
    "    #\n",
    "    # formats axes by setting axis thickness & ticks to lwidth and clearing top/right axes\n",
    "    #\n",
    "    #\n",
    "    axname.spines['bottom'].set_linewidth(lwidth)\n",
    "    axname.tick_params(width=lwidth)\n",
    "    axname.spines['left'].set_linewidth(lwidth)\n",
    "    axname.spines['top'].set_linewidth(0)\n",
    "    axname.spines['right'].set_linewidth(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellset_data_volumes(cs_file, snr_thresh=2, isx_ed_threshold = 5, autosort_snr = 5):\n",
    "    #\n",
    "    # reads cellset specified by cs_file and returns:\n",
    "    # [trace_mtx, raster_mtx, raster_mad_mtx]\n",
    "    #\n",
    "    # trace_mtx is [n,m] matrix of m-cells n-sample calcium signals \n",
    "    # raster_mtx is [n,m] matrix of m-cells n-sample calcium events (0-1) deteted with event_detection_cnmfe()\n",
    "    # raster_mas_mtx is [n,m] matrix of m-cells n-sample calcium events (0-1) deteted with isx.event_detection()\n",
    "    # \n",
    "    # returns accepted/undecided cells only\n",
    "    #\n",
    "    \n",
    "    # read cellset:\n",
    "    cellset = isx.CellSet.read(cs_file)\n",
    "\n",
    "    # event detection:\n",
    "    ed_file = isx.make_output_file_path(cs_file, os.path.dirname(cs_file), 'ED')\n",
    "    if os.path.isfile(ed_file):\n",
    "        os.remove(ed_file)\n",
    "    event_detection_cnmfe(cellset, snr_thresh = snr_thresh, peak_dist = 0.3, peak_width = 0.2)\n",
    "\n",
    "    # isx (MAD) event detection:\n",
    "    isxed_file = isx.make_output_file_path(cs_file, os.path.dirname(cs_file), 'isxED')\n",
    "    if os.path.isfile(isxed_file):\n",
    "        os.remove(isxed_file)\n",
    "    isx.event_detection(cs_file, isxed_file, event_time_ref='maximum', threshold=isx_ed_threshold)\n",
    "\n",
    "    \n",
    "    # build trace, raster, peak data volumes:\n",
    "    trace_volume = build_trace_volume([cs_file])\n",
    "    raster_volume = build_raster_volume([isx.make_output_file_path(cs_file, os.path.dirname(cs_file), 'ED')], sm_win=0)\n",
    "    raster_isx_volume = build_raster_volume([isx.make_output_file_path(cs_file, os.path.dirname(cs_file), 'isxED')], sm_win=0)\n",
    "    #peak_volume = (raster_volume*trace_volume) # rasters with scalar event sizes\n",
    "    #peak_isx_volume = (raster_isx_volume*trace_volume)\n",
    "    #ref_volume = np.copy(trace_volume)\n",
    "    #ref_volume[10:,0] = signal.medfilt(ref_volume[:-10,0],7)\n",
    "\n",
    "    # get length of recording:\n",
    "    #max_t = (isx.Duration.to_usecs(cellset.timing.period)*1e-6) * cellset.timing.num_samples\n",
    "    \n",
    "    # auto accept/reject:\n",
    "    isx.auto_accept_reject(cs_file, isxed_file, filters = [('# Comps', '=', 1), ('Event Rate', '>', 0), ('SNR', '>', autosort_snr)])\n",
    "    \n",
    "\n",
    "    # create mask for excluding rejected cells:\n",
    "    cell_mask = np.ones((cellset.num_cells))\n",
    "    #print(cellset.num_cells)\n",
    "    for i in range(cellset.num_cells):\n",
    "        if cellset.get_cell_status(i) == 'rejected':\n",
    "            cell_mask[i] = 0\n",
    "\n",
    "    accepted_cells = np.nonzero(cell_mask)[0]\n",
    "    \n",
    "    return [trace_volume[0][:,accepted_cells], raster_volume[0][:,accepted_cells], raster_isx_volume[0][:,accepted_cells]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cellset_paths(df):\n",
    "    '''\n",
    "    get_cellset_paths(df, col_dict = {})\n",
    "    \n",
    "    INPUTS:\n",
    "    df <pandas dataframe>: must have these columns: 'data_dir_ca', 'isxd_data_basename'\n",
    "    \n",
    "    OUTPUTS:\n",
    "    out_list <list>: list of isxd cellsets meeting conditions in col_dict\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    for pathname,filename in zip(df.data_dir_ca.values, df.isxd_data_basename.values):\n",
    "        pathname = fix_data_path(pathname)\n",
    "        if os.path.isdir(pathname):\n",
    "            if len([i for i in os.listdir(pathname) if 'pcaica.isxd' in i]): # check base directory for pcaica.isxd cellset\n",
    "                cellset_fn = pathname + [i for i in os.listdir(pathname) if 'pcaica.isxd' in i][0]\n",
    "                out_list.append(cellset_fn)\n",
    "            else: # check for pipeline directory\n",
    "                if len([i for i in os.listdir(pathname) if 'pipeline' in i]):\n",
    "                    pipeline_path = [i for i in os.listdir(pathname) if 'pipeline' in i][0]\n",
    "                    if len([i for i in os.listdir(pathname+pipeline_path) if 'pcaica.isxd' in i]):\n",
    "                        cellset_fn = pathname + pipeline_path + '/' + [i for i in os.listdir(pathname+pipeline_path) if 'pcaica.isxd' in i][0]\n",
    "                        out_list.append(cellset_fn)\n",
    "\n",
    "\n",
    "    \n",
    "    #print([fix_data_path(i) for i in df.data_dir_ca],'\\n')\n",
    "    #print([i for i in df.isxd_data_basename],'\\n')\n",
    "    \n",
    "    return out_list\n",
    "\n",
    "    \n",
    "\n",
    "def get_cellset_timescale(cellset_fn):\n",
    "    #\n",
    "    # returns time vector for traces in cellset_fn\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    cs = isx.CellSet.read(cellset_fn)\n",
    "    per = cs.timing.period.to_usecs()*1e-6\n",
    "    \n",
    "    tvect = np.arange(0,cs.timing.num_samples * per, per)\n",
    "    return tvect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data_path(the_path, base_path = 'ariel', path_sep = '\\\\'):\n",
    "    '''\n",
    "    fix_data_path(thepath, basepath = '/ariel/'):\n",
    "    \n",
    "    INPUTS:\n",
    "    the_path <str>:\n",
    "    base_path <str>:\n",
    "    \n",
    "    OUTPUTS:\n",
    "    out_path <str>:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    path_elements = the_path.split(path_sep)\n",
    "    for idx,i in enumerate(path_elements):  \n",
    "        if i=='science-1':\n",
    "            path_elements[idx] = 'science'\n",
    "        if i=='data':\n",
    "            path_elements[idx] = ''\n",
    "    path_elements = [base_path] + [i for i in path_elements if len(i)]\n",
    "    path_elements = ['/']+[i+'/' for i in path_elements]\n",
    "\n",
    "    return ''.join(path_elements)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frame_lookup(gpio_csv_fn, cellset_fn, sync_chan_name = ' GPIO-1', offset = 0, pulse_thresh =0.55):\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    cs_time = (get_cellset_timescale(cellset_fn))\n",
    "    fs = round(1/cs_time[1], 8)\n",
    "\n",
    "    if os.path.isfile(gpio_fn):\n",
    "        gpio = pd.read_csv(gpio_fn)\n",
    "        sync_df = gpio.loc[gpio[' Channel Name'] == sync_chan_name]\n",
    "        sync_df.reset_index(inplace=True, drop=True)\n",
    "        sync_sub = sync_df[['Time (s)',' Value']]\n",
    "    else:\n",
    "        print('{} file does not exist'.format(gpio_fn))\n",
    "        \n",
    "    \n",
    "    # detect pulse onsets and offsets in gpio:\n",
    "    in_data = sync_df[' Value'].astype(np.float32)\n",
    "    in_time = sync_df['Time (s)'].astype(np.float32)\n",
    "\n",
    "    crossings = np.diff(1 * (in_data >  max(in_data) * pulse_thresh) != 0 )\n",
    "\n",
    "    pulse_onsets = in_time[1:][crossings].values[0::2]\n",
    "    pulse_offsets = in_time[1:][crossings].values[1::2]\n",
    "\n",
    "    start_t = pulse_onsets[0] - offset\n",
    "    stop_t = pulse_onsets[-1] + (1/fs) - offset\n",
    "    \n",
    "    # create behavioral video frame lookup table and map to isxd timebase:\n",
    "    sync_times = np.zeros((len(cs_time), 1))\n",
    "    ons_idx = ((pulse_onsets - offset) * fs).astype(int)\n",
    "    sync_times[ons_idx] = 1\n",
    "    frame_lookup = np.cumsum(sync_times).astype(int)\n",
    "    frame_lookup[np.argmax(frame_lookup)+1:] = 0\n",
    "    \n",
    "    return frame_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_com_entries(com, x_threshs, entry_dir = (0 , 1), t_thresh = 10, detect_win = 5, extract_win = 10):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    com <n x 2 array>\n",
    "    x_threshs <n=2 array>\n",
    "    entry_dir = (0 , 1)<n=2 array>\n",
    "    t_thresh = 10 (int, frames)\n",
    "    detect_win = 5 (int, pixels)\n",
    "    extract_win = 10 (int, frames)\n",
    "\n",
    "    OUTPUTS:\n",
    "    (left_entries, left_exits, right_entries, right_exits)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    outdict = dict()\n",
    "    for t in x_threshs:\n",
    "        outdict[t] = {}\n",
    "        outdict[t]['entries'] = []\n",
    "        outdict[t]['entry frames'] = []\n",
    "        outdict[t]['exits'] = []\n",
    "        outdict[t]['exit frames'] = []\n",
    "    \n",
    "    for t, cross_dir in zip(x_threshs, entry_dir):\n",
    "\n",
    "        thresh_range  = np.arange(t - detect_win, t + detect_win)\n",
    "        crosspnts = np.intersect1d(np.argwhere(com[:,1] > thresh_range[0]), np.argwhere(com[:,1] < thresh_range[-1])  )            \n",
    "        crosspnts = crosspnts[np.diff(crosspnts, prepend = [0]) > t_thresh]\n",
    "        \n",
    "        crossings = []\n",
    "\n",
    "        for p in crosspnts:\n",
    "            the_seg = com[p - extract_win : p + extract_win,:]\n",
    "            crossings.append(the_seg)\n",
    "        crossings = np.asarray(crossings).astype(int)\n",
    "\n",
    "        # distinguish entries from exits:\n",
    "        pop_idx = []\n",
    "        keep_idx = []\n",
    "\n",
    "        for idx, i in enumerate(crossings):\n",
    "            if cross_dir == 0: # right to left entries\n",
    "                if i[0,1] < i[-1,1]:\n",
    "                    pop_idx.append(idx)\n",
    "                else:\n",
    "                    keep_idx.append(idx)\n",
    "            elif cross_dir == 1: # left to right entries\n",
    "                if i[0,1] > i[-1,1]:\n",
    "                    pop_idx.append(idx)\n",
    "                else:\n",
    "                    keep_idx.append(idx)\n",
    "                \n",
    "                \n",
    "        entries = crossings[keep_idx]\n",
    "        exits = crossings[pop_idx]\n",
    "        \n",
    "        outdict[t]['entries'] = entries\n",
    "        outdict[t]['entry frames'] = crosspnts[keep_idx]\n",
    "        outdict[t]['exits'] = exits\n",
    "        outdict[t]['exit frames'] = crosspnts[pop_idx]\n",
    "        \n",
    "    return outdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_aligned_trace_dict(stat_dict, framewin, data_type = 'trace_mtx', x_thresh = (95,230), win = (20,60), event_types = ['entry frames'], shuffle = False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #framewin = (phase_start, phase_stop) # segment of video frames to analyze\n",
    "\n",
    "    aligned_traces_dict =  dict()\n",
    "\n",
    "    for cond in stat_dict.keys():\n",
    "        #print(cond)\n",
    "        datasets = stat_dict[cond].keys()\n",
    "        aligned_traces_dict[cond] = {}\n",
    "        for ds in stat_dict[cond]:\n",
    "            #print('\\t',ds)\n",
    "            the_com = stat_dict[cond][ds]['COM'][framewin[0] : framewin[1], :]\n",
    "            align_dict = find_com_entries(the_com, x_thresh)\n",
    "            aligned_traces_dict[cond][ds] = {}\n",
    "            for align_point in x_thresh:\n",
    "                aligned_traces_dict[cond][ds][align_point] = {}\n",
    "                for the_event_type in event_types:\n",
    "                    aligned_traces_dict[cond][ds][align_point][the_event_type] = {}\n",
    "                    if shuffle == True:\n",
    "                        #print('Using random indices')\n",
    "                        randvect = np.random.choice(np.arange(framewin[0], framewin[1]), size = len(align_dict[align_point][the_event_type]), replace=False )\n",
    "                        align_indices = [np.arange(i-win[0], i+win[1]) for i in randvect]\n",
    "                    else:\n",
    "                        #print('Aligning to triggers')\n",
    "                        align_indices = [np.arange(i-win[0], i+win[1]) for i in align_dict[align_point][the_event_type]]\n",
    "                    avg_mtx = np.mean(stat_dict[cond][ds][data_type][align_indices, :], axis = 0)\n",
    "                    all_trial_mtx = stat_dict[cond][ds][data_type][align_indices, :]\n",
    "                    aligned_traces_dict[cond][ds][align_point][the_event_type]['aligned_mtx'] = all_trial_mtx\n",
    "                    \n",
    "        \n",
    "    return aligned_traces_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### null modulation distribution defs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_mod_index(invect):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    win_1 = (30,80)\n",
    "    win_2 = (0,20)\n",
    "\n",
    "    #print(np.shape(invect))\n",
    "    #print(np.mean(invect[win_1[0]:win_1[1]]))\n",
    "    \n",
    "    mod_idx = np.mean(invect[win_1[0]:win_1[1]]) / np.mean(invect[win_2[0]:win_2[1]])\n",
    "    #mod_idx = np.mean(invect[win_1[0]:win_1[1]])\n",
    "    return mod_idx\n",
    "\n",
    "\n",
    "\n",
    "def make_modulation_null_dist(in_mtx, pre_win, post_win, p_start, p_stop, null_iters = 1000, num_pool = 1, min_subtract=True):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    num_cells = (np.shape(in_mtx)[1])\n",
    "    out_dist = np.zeros((num_cells, null_iters))\n",
    "    \n",
    "    for cell_idx in range(num_cells):\n",
    "        for idx in range(null_iters):\n",
    "            align_pnts = np.random.choice(np.arange(p_start,p_stop), size=num_pool, replace=True)\n",
    "            trace_mtx = np.zeros((num_pool, pre_win+post_win))\n",
    "            for t_idx, pnt in enumerate(align_pnts):\n",
    "                if min_subtract:\n",
    "                    trace_mtx[t_idx,:] = in_mtx[pnt-pre_win : pnt+post_win , cell_idx] - (np.min(in_mtx[pnt-pre_win : pnt+post_win , cell_idx]))\n",
    "                else:\n",
    "                    trace_mtx[t_idx,:] = in_mtx[pnt-pre_win : pnt+post_win , cell_idx]\n",
    "            out_dist[cell_idx, idx] = compute_mod_index(np.mean(trace_mtx, axis=0))\n",
    "    \n",
    "    return out_dist\n",
    "        \n",
    "    \n",
    "\n",
    "def prob_from_dist(test_val, comp_dist, hist_range = (-20,20)):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #max_val = max((test_val, max(comp_dist)))\n",
    "    #min_val = min((test_val, min(comp_dist)))\n",
    "    h,hbins = np.histogram(comp_dist, bins = 100, range = hist_range)\n",
    "    hdist = stats.rv_histogram((h, hbins) )\n",
    "    #modulation_p_dist.append(min((hdist.cdf(np.median(cell_stat)), 1-hdist.cdf(np.median(cell_stat)))) )\n",
    "    return min((hdist.cdf(test_val), 1-hdist.cdf(test_val))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = pickle.load(open('frame_aligned_data_20200415.pkl' , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_dict = {'habituate':(500,5500), 'preference':(6500,17500), 'novelty':(18500,18500+11500) }\n",
    "null_phase = 'habituate'\n",
    "sides = (95,230)\n",
    "events = ['entry frames']\n",
    "dat_type = 'trace_mtx'\n",
    "do_min_subtract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_out = dict.fromkeys(phase_dict.keys())\n",
    "#dicts_out = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'habituate': None, 'preference': None, 'novelty': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pooling loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OM047\n",
      "habituate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmiller/Applications/anaconda3/envs/astellasenv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mmiller/Applications/anaconda3/envs/astellasenv/lib/python3.6/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "preference\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "novelty\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "OM051\n",
      "habituate\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "preference\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "novelty\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "OM035\n",
      "habituate\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "preference\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "novelty\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "OM045\n",
      "habituate\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "preference\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "novelty\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "OM038\n",
      "habituate\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "preference\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "novelty\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "OM042\n",
      "habituate\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "preference\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "novelty\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "OM040\n",
      "habituate\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "preference\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "novelty\n",
      "\t 95 dict_keys(['aligned_mtx'])\n",
      "\t 95 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n",
      "\t 230 dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])\n"
     ]
    }
   ],
   "source": [
    "for the_cond in stat_dict.keys():\n",
    "    for the_subj in stat_dict[the_cond].keys():\n",
    "        print(the_subj)\n",
    "        for phase in phase_dict:\n",
    "            phase_start = phase_dict[phase][0]\n",
    "            phase_stop = phase_dict[phase][1]\n",
    "            aligned_traces_dict = make_aligned_trace_dict(stat_dict, framewin=phase_dict[phase])\n",
    "            print(phase)\n",
    "            for the_side in sides:\n",
    "                the_null_trace_mtx = stat_dict[the_cond][the_subj][dat_type]\n",
    "\n",
    "                if len(aligned_traces_dict[the_cond][the_subj][the_side]['entry frames']['aligned_mtx'].shape) < 3:\n",
    "                    aligned_traces_dict[the_cond][the_subj][the_side]['entry frames']['modulation'] = []\n",
    "                    aligned_traces_dict[the_cond][the_subj][the_side]['entry frames']['modulation prob'] = []\n",
    "                    continue\n",
    "                num_trials = aligned_traces_dict[the_cond][the_subj][the_side]['entry frames']['aligned_mtx'].shape[0]\n",
    "                num_cells = aligned_traces_dict[the_cond][the_subj][the_side]['entry frames']['aligned_mtx'].shape[2]\n",
    "                null_dist = make_modulation_null_dist(the_null_trace_mtx, 20, 60, null_iters=100, \n",
    "                                                      p_start = phase_dict[null_phase][0], p_stop = phase_dict[null_phase][1], \n",
    "                                                      num_pool = num_trials, min_subtract = do_min_subtract)\n",
    "                \n",
    "                stat_mtx = np.mean(aligned_traces_dict[the_cond][the_subj][the_side]['entry frames']['aligned_mtx'], axis=0)\n",
    "                mod_mtx = []\n",
    "                for i in range(np.shape(stat_mtx)[1]):\n",
    "                    if do_min_subtract is True:\n",
    "                        min_subtracted = (stat_mtx.transpose()[i]) - (np.min(stat_mtx.transpose()[i]))\n",
    "                    else:\n",
    "                        min_subtracted = (stat_mtx.transpose()[i])\n",
    "                    mod_mtx.append(compute_mod_index(min_subtracted))\n",
    "                mod_p = np.asarray([prob_from_dist(i,j, hist_range=(0,10)) for i,j in zip(mod_mtx, null_dist)])\n",
    "                #print('\\t',the_side, aligned_traces_dict[the_cond][the_subj][the_side]['entry frames'].keys())                \n",
    "                aligned_traces_dict[the_cond][the_subj][the_side]['entry frames']['modulation'] = mod_mtx\n",
    "                aligned_traces_dict[the_cond][the_subj][the_side]['entry frames']['modulation prob'] = mod_p\n",
    "                #print('\\t',the_side, aligned_traces_dict[the_cond][the_subj][the_side]['entry frames'].keys())\n",
    "\n",
    "                #print('\\t',the_side, aligned_traces_dict[the_cond][the_subj][the_side]['entry frames']['modulation prob'])\n",
    "            #print('\\t',the_side, aligned_traces_dict[the_cond][the_subj][the_side]['entry frames'].keys())\n",
    "            dicts_out[phase] = aligned_traces_dict\n",
    "            #dict_out.append(aligned_traces_dict)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['habituate', 'preference', 'novelty'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['aligned_mtx'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dicts_out.keys())\n",
    "#print((dicts_out['habituate']))\n",
    "(dicts_out['novelty']['FMR1CTRL']['OM051'][95]['entry frames'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aligned_mtx', 'modulation', 'modulation prob'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dicts_out)\n",
    "dicts_out[0]['FMR1CTRL']['OM047'][95]['entry frames'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
