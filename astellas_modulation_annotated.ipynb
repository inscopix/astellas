{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "import bisect\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import transform\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import scipy.signal as signal\n",
    "import scipy.interpolate as interpolate\n",
    "from scipy import ndimage as nd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "from skimage import transform\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import isx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butterworth(sig,cutoff,fs,filt_order,filt_type):\n",
    "    #\n",
    "    # applies zero-phase digital butterworth filter to data in sig\n",
    "    # if sig is a matrix, data in its columns are filtered\n",
    "    #\n",
    "    # cutoff is filter's critical frequency in Hz, or n=2 tuple of freqs for bandpass filters\n",
    "    # fs is sampling rate\n",
    "    # filt_order is integer filter order\n",
    "    # filt_type is 'lowpass','highpass', 'bandpass', or 'bandstop' \n",
    "    # \n",
    "    # returns filtered signal \n",
    "    #\n",
    "    \n",
    "    nyq = 0.5 * fs\n",
    "    norm_cutoff =cutoff/nyq\n",
    "    b,a = signal.butter(filt_order,norm_cutoff,btype=filt_type,analog=False)\n",
    "    \n",
    "    if sig.ndim > 1:\n",
    "        filt_sig = signal.filtfilt(b,a,sig,axis=1)\n",
    "    elif sig.ndim == 1:\n",
    "        filt_sig = signal.filtfilt(b,a,sig)\n",
    "    else:\n",
    "        filt_sig = []\n",
    "    \n",
    "    return filt_sig\n",
    "\n",
    "def median_filter_matrix(inmtx,winsize=3):\n",
    "    #\n",
    "    # median filters each column of matrix with kernal width = winsize\n",
    "    #\n",
    "    # returns filtered matrix\n",
    "    #\n",
    "    \n",
    "    filtmtx = np.zeros_like(inmtx)\n",
    "    \n",
    "    for i in range(inmtx.shape[0]):\n",
    "        filtmtx[i] = signal.medfilt(inmtx[i],winsize)\n",
    "    return filtmtx\n",
    "\n",
    "def otsu(x):\n",
    "    # returns threshold estimated via Otsu's method\n",
    "    # x needs to be 1-d list\n",
    "    nbins = 500\n",
    "    logx = np.log(x)\n",
    "    logx = np.asarray([i for i in logx if np.isfinite(i)])\n",
    "    \n",
    "    minim = min(logx)\n",
    "    maxim = max(logx)\n",
    "    logx = (logx-minim)/(maxim-minim)\n",
    "    \n",
    "    xhist = np.histogram(logx, bins=nbins)\n",
    "    xbins = xhist[1]\n",
    "    xbins = xbins[1:]\n",
    "    xcounts = xhist[0] / sum(xhist[0])\n",
    "\n",
    "    maximum = 0\n",
    "    thresh = 0\n",
    "    \n",
    "    for t in range(len(xcounts)):\n",
    "        w0 = sum(xcounts[:t])\n",
    "        w1 = sum(xcounts) - w0\n",
    "        if w0 == 0 or w1 == 0:\n",
    "            #print('continuing...')\n",
    "            continue\n",
    "        mu0 = sum(xbins[:t]*xcounts[:t]) / w0\n",
    "        mu1 = sum(xbins[t:]*xcounts[t:]) / w1\n",
    "        sigB = w0 * w1 * ((mu0 - mu1) * (mu0 - mu1))\n",
    "        if sigB >= maximum:\n",
    "            maximum = sigB\n",
    "            thresh = xbins[t]\n",
    "    \n",
    "    th = thresh*(maxim-minim) + minim\n",
    "    th = np.exp(th)\n",
    "    #th = np.power(10,th)\n",
    "    thresh = th\n",
    "\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def register_signals(ref_time,in_dat,in_time):\n",
    "    #\n",
    "    # Direct alignment of signals that were sampled at diffrent rates\n",
    "    # resamples in_dat to len(ref_time) by aligning timestamps in ref_time with timestamps in in_time\n",
    "    # in_time is vector of timestamps for data in in_dat\n",
    "    #\n",
    "    # len(ref_time) < len(in_time)\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    resamp_dat = np.zeros_like(ref_time)\n",
    "\n",
    "    for t in range(len(ref_time)):\n",
    "        dat = in_dat[bisect.bisect(in_time,ref_time[t])-1]\n",
    "        resamp_dat[t] = dat\n",
    "    return resamp_dat\n",
    "    \n",
    "def segment(x,thresh):\n",
    "    # returns onsets & offset indices of impulses that exceed thresh in vector x\n",
    "    xsub = x - thresh\n",
    "    xdiff = (xsub[:-1]*xsub[1:]) < 0\n",
    "    edges = np.where(xdiff == 1) # indices at onsets/offsets\n",
    "    edges = list(edges[0])\n",
    "    \n",
    "    x[0:5] = 0\n",
    "    \n",
    "    if np.mean(np.diff(x[edges[0]-2:edges[0]+2])) < 0: # delete first impulse if offset\n",
    "        print('first impulse is offset')\n",
    "        edges.pop(0)\n",
    "    if np.mean(np.diff(x[edges[-1]-2:edges[-1]+2])) > 0: # delete last impulse if onset\n",
    "        print('last impulse is onset')\n",
    "        edges.pop(-1)\n",
    "    onsets = edges[0::2]\n",
    "    offsets = edges[1::2]\n",
    "    \n",
    "    if len(onsets) > len(offsets):\n",
    "        onsets.pop(-1)\n",
    "    \n",
    "    return [onsets,offsets]\n",
    "\n",
    "def extractsegments(x,onsets,offsets,win):\n",
    "    # returns list of slices from x[onsets[i]-win:offsets[i]+win]\n",
    "    \n",
    "    win = int(win)\n",
    "    if onsets[0] - win < 1 or offsets[-1] > len(x):\n",
    "        print('Edge Syllable')\n",
    "        return []\n",
    "    else:\n",
    "        segs = [x[onsets[i]-win : offsets[i]+win] for i in range(len(onsets))]\n",
    "        return segs\n",
    "    \n",
    "def filtersegments(segs,minlen,maxlen):\n",
    "    # deletes elements of segs with maxlen < len() < minlen\n",
    "    return [i for i in segs if len(i) > minlen and len(i) < maxlen]\n",
    "    \n",
    "def twoaxis(axname,lwidth=2):\n",
    "    #\n",
    "    # formats axes by setting axis thickness & ticks to lwidth and clearing top/right axes\n",
    "    #\n",
    "    #\n",
    "    axname.spines['bottom'].set_linewidth(lwidth)\n",
    "    axname.tick_params(width=lwidth)\n",
    "    axname.spines['left'].set_linewidth(lwidth)\n",
    "    axname.spines['top'].set_linewidth(0)\n",
    "    axname.spines['right'].set_linewidth(0)\n",
    "    \n",
    "# functions for object detection:\n",
    "\n",
    "def subtract_img_background(input_img):\n",
    "    #\n",
    "    # Uses pixel dilation to adaptively filter nonuniform background from objects in input_img\n",
    "    #\n",
    "    # input image is NxM  matrix of pixel intensities \n",
    "    #\n",
    "    # returns filtered image.\n",
    "    #\n",
    "    # Adapted from skimage 'Filtering regional maxima'. Requires : \n",
    "    # import numpy as np\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # from scipy.ndimage import gaussian_filter\n",
    "    # from skimage.filters import threshold_otsu\n",
    "    # #from skimage import img_as_float\n",
    "    # from skimage.morphology import reconstruction\n",
    "    #\n",
    "    #\n",
    "    img = img_as_float(input_img)\n",
    "    img_g = gaussian_filter(img,1)\n",
    "\n",
    "    h = threshold_otsu(img_g)\n",
    "\n",
    "    seed = img_g - h\n",
    "    mask = img_g\n",
    "\n",
    "    dilated = reconstruction(seed,mask,method='dilation')\n",
    "\n",
    "    return img_g-dilated\n",
    "\n",
    "def norm_image(in_img):\n",
    "    #\n",
    "    # returns in_img normalized to [0-255]\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    i_min = np.min(in_img)\n",
    "    i_max = np.max(in_img)\n",
    "    \n",
    "    norm_img = 255 * ((in_img + abs(i_min) / i_max))\n",
    "    \n",
    "    return norm_img\n",
    "\n",
    "def estimate_num_cells(in_img):\n",
    "    #\n",
    "    # Estimates number of gaussian-like blobs in in_img using laplacian of gaussians (LoG)\n",
    "    #\n",
    "    # \n",
    "    # in_img is NxM matrix of pixel intensities\n",
    "    # in_img is probably maximum or other projection of motion corrected or df/f movie\n",
    "    #\n",
    "    # requires skimage.feature.blob_log()\n",
    "    #\n",
    "    # returns N x 3 matrix, N=num_blobs, out[0] = y coordinates, out[1] = x coordinates, out[2] = blob radius \n",
    "    #\n",
    "    \n",
    "    # log_params:\n",
    "    min_sigma = 2\n",
    "    max_sigma = 20\n",
    "    num_sigma = int((max_sigma-min_sigma))\n",
    "    threshold = 1\n",
    "    overlap =0.1\n",
    "    \n",
    "    img_background_subtract = subtract_img_background(in_img)\n",
    "    img_norm = norm_image(img_background_subtract)\n",
    "    objs = blob_log(img_norm,min_sigma = min_sigma,max_sigma=max_sigma,num_sigma=num_sigma,threshold=threshold,overlap=overlap)\n",
    "    \n",
    "    return objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nans(indat):\n",
    "    #\n",
    "    # returns the number of NaN values in indat\n",
    "    # \n",
    "    \n",
    "    return np.isnan(indat).sum()\n",
    "\n",
    "def events_to_raster(event_times,timing,smooth_win=0):\n",
    "    #\n",
    "    # returns raster and length vector of event_times, optionally smoothed with gaussian of smooth_win number of pnts\n",
    "    #\n",
    "    # timing is <class 'isx.core.Timing'> from EventSet\n",
    "    #\n",
    "    # Example usage: offs,amps = events_1.get_cell_data(0)\n",
    "    # [rast,tvect] = events_to_raster(offs,events_1.timing,0)\n",
    "    # plot tvect vs rast\n",
    "    #\n",
    "    \n",
    "    samp_vect = [i for i in range(timing.num_samples)]\n",
    "    event_times_samps = event_times / timing.period.to_usecs()\n",
    "    raster = np.zeros_like(samp_vect)\n",
    "    if len(event_times) == 0:\n",
    "        raster = [np.nan for i in raster]        \n",
    "        return[raster,samp_vect]\n",
    "    else:    \n",
    "        raster[event_times_samps.astype(int)] = 1\n",
    "    \n",
    "        if smooth_win > 0:\n",
    "            win = signal.gaussian(int(smooth_win)*10,smooth_win)\n",
    "            raster_sm = np.convolve(raster,win,mode='same')\n",
    "            raster = raster_sm\n",
    "        \n",
    "        return [raster,samp_vect]\n",
    "\n",
    "def rasters_from_eventset(eventset,sm_win=0):\n",
    "    #\n",
    "    # returns numcell X sample matrix of rasters derived from eventset\n",
    "    #\n",
    "    # optionally smooths rasters with gaussian window with std deviation = sm_win samples\n",
    "    #\n",
    "    \n",
    "    offs,amps = eventset.get_cell_data(0)\n",
    "    raster,raster_x = events_to_raster(offs,eventset.timing,sm_win)\n",
    "    outmtx = np.zeros([len(raster),eventset.num_cells])\n",
    "    for cellnum in range(eventset.num_cells):\n",
    "        offs,amps = eventset.get_cell_data(cellnum)\n",
    "        raster,raster_x = events_to_raster(offs,eventset.timing,sm_win)\n",
    "        outmtx[:,cellnum] = raster\n",
    "    \n",
    "    return outmtx\n",
    " \n",
    "def traces_from_cellset(cellset,nrm=0):\n",
    "    #\n",
    "    # returns numcells x max(tstamps) matrix of traces from cellset\n",
    "    # centers (mean subtracts) each trace if nrm==1\n",
    "    #\n",
    "    \n",
    "    tracelen = 0\n",
    "    for i in range(cellset.num_cells): # find length of traces for initialization:\n",
    "        if len(cellset.get_cell_trace_data(0)) > tracelen:\n",
    "            tracelen = len(cellset.get_cell_trace_data(0))\n",
    "            break\n",
    "    outmtx = np.zeros([tracelen,cellset.num_cells]) # init output\n",
    "    for i in range(cellset.num_cells): # populate output with traces from cellsets:\n",
    "        if nrm == 1 and sum(abs(cellset.get_cell_trace_data(i)))>0:\n",
    "            outmtx[:,i] = cellset.get_cell_trace_data(i) - np.nanmean(cellset.get_cell_trace_data(i))\n",
    "        else:    \n",
    "            outmtx[:,i] = cellset.get_cell_trace_data(i)\n",
    "    return outmtx\n",
    "\n",
    "def images_from_cellset(cellset):\n",
    "    #\n",
    "    # returns numcells x max(tstamps) matrix of images from cellset\n",
    "    #\n",
    "    img = cellset.get_cell_image_data(0)\n",
    "    \n",
    "    outmtx = np.zeros([img.shape[0],img.shape[1],cellset.num_cells]) # init output\n",
    "    for i in range(cellset.num_cells): # populate output with traces from cellsets:\n",
    "        outmtx[:,:,i] = cellset.get_cell_image_data(i)\n",
    "    return outmtx\n",
    "\n",
    "\n",
    "def remove_nan_rows(inmtx_list):\n",
    "    #\n",
    "    # Detects rows with missing data points (NaNs) in each matrix in inmtx_list. Removes rows from all elements\n",
    "    # of inmtx_list\n",
    "    #\n",
    "    \n",
    "    nanrows = []\n",
    "    outlist = []\n",
    "    for mtx in inmtx_list:\n",
    "        livecols = np.where(np.all(np.isnan(mtx),axis=0)==False)[0] # find columns that contain data\n",
    "        thenanrows = np.where(np.any(np.isnan(mtx[:,livecols]),axis=1))[0] # find rows with NaNs\n",
    "        nanrows.append(thenanrows)\n",
    "    del_these_rows = [i for lst in nanrows for i in lst] # flattens lists in nanrows\n",
    "    \n",
    "    for mtx in inmtx_list:\n",
    "        outmtx = np.delete(mtx,del_these_rows,axis=0)\n",
    "        #outlist.append(np.delete(mtx,del_these_rows,axis=0))\n",
    "        outlist.append(outmtx)\n",
    "    \n",
    "    return outlist\n",
    "\n",
    "def build_trace_volume(cellset_list):\n",
    "    #\n",
    "    # assembles [p,l,c] matrix of traces from list of LR cellsets. p is number of planes/cellsets, l is data\n",
    "    # length, c is number of cells.\n",
    "    #\n",
    "    # removes dropped/cropped frames from all cellsets' data\n",
    "    # removes additional rows with missing data from all cellsets' data\n",
    "    #\n",
    "    # if traces are not equal length, crops ends to minimum length\n",
    "    #\n",
    "    # returns trace_matrix\n",
    "    # \n",
    "    \n",
    "    remove_frames = np.array([],dtype=int)\n",
    "    tracemtx_list = []\n",
    "    for l in cellset_list:\n",
    "        cellset = isx.CellSet.read(l)\n",
    "        remove_frames = np.append(remove_frames,cellset.timing.dropped)\n",
    "        remove_frames = np.append(remove_frames,cellset.timing.cropped)\n",
    "        tracemtx_list.append(traces_from_cellset(cellset,nrm=1))\n",
    "    \n",
    "    clean_list = []\n",
    "    for m in tracemtx_list:\n",
    "        outmtx = np.delete(m,remove_frames.flatten().astype(int),axis=0)\n",
    "        clean_list.append(outmtx)    \n",
    "    \n",
    "    clean_list_nonan = remove_nan_rows(clean_list)\n",
    "    npnts = min([len(i) for i in clean_list_nonan])\n",
    "    out_list = [i[0:npnts,:] for i in clean_list_nonan]\n",
    "    trace_vol = np.stack(out_list,axis=0)\n",
    "    \n",
    "    return trace_vol\n",
    "\n",
    "def build_img_volume(cellset_list):\n",
    "    #\n",
    "    # assembles [p,x,y,c] matrix of images from list of LR cellsets. p is number of planes/cellsets, l is data\n",
    "    # length, c is number of cells.\n",
    "    #\n",
    "    # returns image matrix\n",
    "    #\n",
    "\n",
    "    mtx_list = []\n",
    "    for l in cellset_list:\n",
    "        cellset = isx.CellSet.read(l)\n",
    "        mtx_list.append(images_from_cellset(cellset))\n",
    "    \n",
    "    img_vol = np.stack(mtx_list,axis=0)\n",
    "    \n",
    "    return img_vol\n",
    "\n",
    "def build_raster_volume(eventset_list,sm_win=0):\n",
    "    #\n",
    "    # assembles [p,x,c] matrix of rasters from list of LR cellsets. p is number of planes/cellsets, x is data\n",
    "    # length, c is number of cells.\n",
    "    #\n",
    "    # optionally smooths rasters with gaussian with std = sm_win samples \n",
    "    #\n",
    "    # returns raster matrix\n",
    "    #\n",
    "\n",
    "    mtx_list = []\n",
    "    for l in eventset_list:\n",
    "        eventset = isx.EventSet.read(l)\n",
    "        mtx_list.append(rasters_from_eventset(eventset,sm_win))\n",
    "    \n",
    "    rast_vol = np.stack(mtx_list,axis=0)\n",
    "    \n",
    "    return rast_vol\n",
    "    \n",
    "def find_LR_planes(inmtx):\n",
    "    #\n",
    "    # detects plane boundaries in Longitudinally registered matrix \n",
    "    #\n",
    "    # returns list of start and stop points for each plane, e.g for 3 planes: [[0:100],[101:200],[201:300]]\n",
    "    #\n",
    "    \n",
    "    outpnts = []\n",
    "    start_p = 0\n",
    "    for p in range(inmtx.shape[0]): # each plane\n",
    "        if p == inmtx.shape[0]-1:\n",
    "            stop_p = inmtx.shape[2]\n",
    "        #elif len(np.nonzero(np.isnan(inmtx[p,0,start_p:]))):\n",
    "        elif np.shape(np.nonzero(np.isnan(inmtx[p,0,start_p:])))[1]:\n",
    "            print(p)\n",
    "            #print(np.isnan(inmtx[p,0,start_p:]))\n",
    "            print(np.shape(np.nonzero(np.isnan(inmtx[p,0,start_p:]))))\n",
    "            print(np.shape(np.nonzero(np.isnan(inmtx[p,0,start_p:])))[1])\n",
    "            stop_p = np.min(np.nonzero(np.isnan(inmtx[p,0,start_p:])))+start_p\n",
    "        else:\n",
    "            stop_p = inmtx.shape[2]\n",
    "        outpnts.append([start_p,stop_p-1])\n",
    "        start_p = stop_p\n",
    "        \n",
    "    return outpnts\n",
    "\n",
    "def corr_dist_1d_volume(inmtx,bkpnts):\n",
    "#\n",
    "# returns distrubution of trace x trace correlations for signals within a plane\n",
    "#\n",
    "# inmtx is matrix of union cellsets with dimensions [num_planes,data_length,num_signals]\n",
    "# bkpnts is array of endpoints that separate imaging planes, e.g. if bkpnts = [100,200,500],\n",
    "# units IDed on plane 1 are inmtx[0,:,0:100], units IDed on plane 2 are inmtx[1,:,101:200], etc\n",
    "# Bkpnts should be same length of np.shape(inmtx)[0]\n",
    "#\n",
    "# to call this function on first plane in matrix, use corr_dist_1d_volume(mtx[0:1,:,:])\n",
    "# to call this function on second plane in matrix, use corr_dist_1d_volume(mtx[1:2,:,:]) etc\n",
    "#\n",
    "    corrlist = []\n",
    "    \n",
    "    pnt_i = 0\n",
    "    startpnt = 0\n",
    "    for plane in inmtx:\n",
    "        subplane = plane[:,startpnt:bkpnts[pnt_i]]\n",
    "        #print(np.shape(subplane),end='')\n",
    "        corrmtx = (np.corrcoef(subplane,rowvar=False))\n",
    "        corrs = corrmtx[np.triu_indices(corrmtx.shape[0],k=1)]\n",
    "        corrlist.append(corrs)\n",
    "        \n",
    "        startpnt = bkpnts[pnt_i]+1\n",
    "        pnt_i += 1\n",
    "        \n",
    "    #return np.concatenate(corrlist)\n",
    "    return corrlist\n",
    "\n",
    "def corr_dist_z_volume(inmtx,bkpnts):\n",
    "#\n",
    "# returns list of distrubutions of trace x trace correlations for distinct xy signals between planes.\n",
    "# also returns list of indices of planes analyzed to serve as a lookup table\n",
    "#\n",
    "# inmtx is matrix of union cellsets with dimensions [num_planes,data_length,num_signals]\n",
    "# bkpnts is array of start-endpoints that separate imaging planes, e.g. if bkpnts = [[0,100],[101,200],[201,500]],\n",
    "# units IDed on plane 1 are inmtx[0,:,0:100], units IDed on plane 2 are inmtx[1,:,101:200], etc\n",
    "# Bkpnts should be same length of np.shape(inmtx)[0]\n",
    "#\n",
    "\n",
    "    corrlist = []\n",
    "    corrmtxlist = []\n",
    "    \n",
    "    plane_list = list(range(np.shape(inmtx)[0]))\n",
    "    plane_combos = list(itl.combinations(plane_list,2))\n",
    "    \n",
    "    for f in plane_combos:\n",
    "        subplane1 = inmtx[f[0],:,bkpnts[f[0]][0]:bkpnts[f[0]][1]]\n",
    "        subplane2 = inmtx[f[1],:,bkpnts[f[1]][0]:bkpnts[f[1]][1]]\n",
    "        mergeplane = np.concatenate([subplane1,subplane2],axis=1)\n",
    "        \n",
    "        corrmtx = (np.corrcoef(mergeplane,rowvar=False))        \n",
    "        corrs = corrmtx[0:bkpnts[f[0]][1]-bkpnts[f[0]][0],bkpnts[f[0]][1]-bkpnts[f[0]][0]+1:len(corrmtx)]\n",
    "        corrmtxlist.append(corrs)\n",
    "        corrlist.append(np.ravel(corrs))\n",
    "        \n",
    "    return [corrlist,corrmtxlist,plane_combos]\n",
    "\n",
    "def aligned_z_corrs(data_volume,threshs = []):\n",
    "    #\n",
    "    # returns distribution of z-correlations in data_volume\n",
    "    # Optionally makes retain/reject decision for each element based on threshs\n",
    "    #\n",
    "    \n",
    "    plane_list = list(range(np.shape(data_volume)[0]))\n",
    "    plane_combos = list(itl.combinations(plane_list,2))\n",
    "    \n",
    "    datacorrs = []\n",
    "    \n",
    "    if len(threshs) > 0:\n",
    "        decision_array = np.zeros([trace_volume.shape[2],1])\n",
    "        decision_planes = [[] for i in decision_array]\n",
    "        splitcorrs = []\n",
    "    \n",
    "    for i in range(data_volume.shape[2]): # each cell\n",
    "        combo_i = 0\n",
    "        for f in plane_combos: # each pairwise plane set\n",
    "            #corrthresh = corr_threshs[f[0],f[1]]\n",
    "            if len(threshs) > 0:\n",
    "                corrthresh = corr_threshs[combo_i]\n",
    "                combo_i += 1\n",
    "                #print(corrthresh)\n",
    "            trace1 = data_volume[f[0],:,i]\n",
    "            trace2 = data_volume[f[1],:,i]\n",
    "        \n",
    "            if np.isnan(trace1).sum() == 0 and np.isnan(trace2).sum() == 0: # if cell has signal on multiple planes then split/merge:\n",
    "                thecorr = np.corrcoef(trace1,trace2)[0][1]\n",
    "                datacorrs.append(thecorr)\n",
    "                if len(threshs) > 0:\n",
    "                    if thecorr < corrthresh: # split cells\n",
    "                        decision_array[i] = 1\n",
    "                        decision_planes[i].append(f)\n",
    "                        splitcorrs.append(round(thecorr,2))\n",
    "\n",
    "    if len(threshs) > 0:\n",
    "        return [datacorrs,decision_array,decision_planes,splitcorrs]\n",
    "    else:\n",
    "        return datacorrs\n",
    "    \n",
    "def write_1d_cellset(fn,trace_data,img_data,cell_names,spacing,timing):\n",
    "    #\n",
    "    # writes .isxd cellset file to filename = fn\n",
    "    # trace_data is [n,c] shaped matrix of activity traces. n = trace length, c = number of cells\n",
    "    # img_data is [x,y,c] shaped matrix of cell images. x and y = image dimensions, c = number of cells\n",
    "    #\n",
    "    # cell_names is list of strings to name each cell. If this is empty, cells are named sequentially.\n",
    "    # \n",
    "    # spacing and timing are given by data movies and describe image size and activity (trace) sampling info\n",
    "    #\n",
    "    # overwrites existing cellsets with same names without warning\n",
    "    #\n",
    "    \n",
    "    if os.path.exists(fn) is False:\n",
    "        out_cellset = isx.CellSet.write(fn,timing,spacing)\n",
    "    elif os.path.exists(fn) is True:\n",
    "        os.remove(fn)\n",
    "        out_cellset = isx.CellSet.write(fn,timing,spacing)\n",
    "        \n",
    "    for i in range(trace_data.shape[1]): # each cell        \n",
    "        if len(cell_names) == 0:\n",
    "            c_name = 'C{}'.format(i)\n",
    "            \n",
    "        else:\n",
    "            c_name = cell_names[i]\n",
    "        out_cellset.set_cell_data(i,img_data[:,:,i].astype(np.float32),trace_data[:,i].astype(np.float32),c_name)\n",
    "    \n",
    "    out_cellset.flush()\n",
    "    \n",
    "def sensitivity_precision(data_volume,winsize = 5):\n",
    "    #\n",
    "    # returns sensitivity and precision calcs for each element n [:,:,n] in data_volume\n",
    "    #\n",
    "    # data_volume[,n,:] are event traces over time (e.g. event trains / rasters)\n",
    "    # data_volume[n,:,:] are reference and test sets\n",
    "    #\n",
    "    # winsize is integer size of window around events\n",
    "    #\n",
    "    \n",
    "    conv_vect = np.ones((1,winsize))[0]\n",
    "    \n",
    "    sens = []\n",
    "    prec = []\n",
    "    \n",
    "    for i in range(data_volume.shape[2]): # each element [:,:,i]\n",
    "        ref_trace = data_volume[0,:,i]\n",
    "        test_trace = data_volume[1,:,i]\n",
    "        if np.isnan(ref_trace).sum() == 0 and np.isnan(test_trace).sum() == 0: # if cell has signal on both planes:\n",
    "            # detected (test) events:\n",
    "            sm_rast_det = np.convolve(test_trace,conv_vect,mode='same')\n",
    "            sm_rast_det[sm_rast_det > 0] = 1\n",
    "            det_pos_pnts = np.nonzero(sm_rast_det)[0] # points around test events\n",
    "            det_neg = [i for i in range(len(ref_trace)) if i not in det_pos_pnts] # points around test negatives\n",
    "            \n",
    "            true_pos = (sum(ref_trace[det_pos_pnts]))\n",
    "            false_pos = max([sum(test_trace) - true_pos, 0])\n",
    "            false_neg = sum(ref_trace[det_neg])\n",
    "            #true_neg = len([i for i in ref_neg if i in det_neg])\n",
    "            \n",
    "            prec.append(true_pos / (true_pos + false_pos)) # precision: fraction of correctly detected events among all detected events\n",
    "            sens.append(true_pos / (true_pos + false_neg)) # sensitivity: fraction of events detected\n",
    "            \n",
    "\n",
    "    return [sens,prec] \n",
    "    \n",
    "def event_detection_cnmfe(cellset, snr_thresh = 1, peak_dist = 0.25, peak_width = 0.4):\n",
    "    #\n",
    "    # Applies scipy.signal.findpeaks() to identify events in CNMF-E or PCA-ICA cellset traces and writes isx.Eventset\n",
    "    #\n",
    "    # Designed to detect events in cnmfe cellsets but also applicable to PCA-ICA cellsets\n",
    "    #\n",
    "    # Computes threshold for each trace as [snr_thresh * (abs(median_abs_deviation / median) * median_abs_deviation)] \n",
    "    #\n",
    "    # \n",
    "    #\n",
    "    \n",
    "    ed_file = isx.make_output_file_path(cellset.file_path, os.path.dirname(cellset.file_path), 'ED')\n",
    "    if os.path.isfile(ed_file):\n",
    "        os.remove(ed_file)\n",
    "    cellnames = [cellset.get_cell_name(i) for i in range(cellset.num_cells)]    \n",
    "    ed_set = isx.EventSet.write(ed_file, cellset.timing, cellnames)\n",
    "    \n",
    "    cellset_fs = round((1 / (cellset.timing.period.to_usecs() * 1e-6) ),3)\n",
    "    #print(cellset_fs)\n",
    "    pdist = int(peak_dist * cellset_fs)\n",
    "    pwidth = int(peak_width * cellset_fs)\n",
    "    \n",
    "    for i in range(cellset.num_cells):\n",
    "        the_trace = cellset.get_cell_trace_data(i)\n",
    "\n",
    "        thresh_mult = snr_thresh * abs(stats.median_absolute_deviation(the_trace) / np.median(the_trace))\n",
    "        peak_thresh = thresh_mult * abs(stats.median_absolute_deviation(the_trace))        \n",
    "        \n",
    "        peaks = signal.find_peaks(the_trace, peak_thresh, distance= pdist, width= pwidth)[0]\n",
    "        ed_set.set_cell_data(i, 1e6*(peaks/cellset_fs), the_trace[peaks])\n",
    "    ed_set.flush()\n",
    "    return ed_set.file_path\n",
    "    \n",
    "    \n",
    "def cellset_qc(cellset_fn, eventset_fn, filters):\n",
    "    #\n",
    "    # applies conditions to accept/reject cells in cellset based on eventset\n",
    "    #\n",
    "    # filters format = [('SNR', '>', 5), ('Event Rate', '>', 0.0015), ('# Comps', '=', 1)]\n",
    "    #\n",
    "    \n",
    "    cellset = isx.CellSet.read(cellset_fn,read_only=False)\n",
    "    if os.path.isfile(eventset_fn):\n",
    "        eventset = isx.EventSet.read(eventset_fn)\n",
    "    else: \n",
    "        event_detection_cnmfe(cellset)\n",
    "        eventset = isx.EventSet.read(eventset_fn)\n",
    "    metric_fn = isx.make_output_file_path(cellset_fn, os.path.dirname(cellset_fn), 'METRICS')\n",
    "    if os.path.isfile(metric_fn):\n",
    "        os.remove(metric_fn)\n",
    "    isx.cell_metrics(cellset_fn, eventset_fn, metric_fn)\n",
    "    metrics_df = pd.read_csv(metric_fn)\n",
    "    metrics_df.columns = [i.lower() for i in metrics_df.columns]\n",
    "    \n",
    "    cell_rej = []\n",
    "    tmp_rej = []\n",
    "    for i in filters:\n",
    "        if i[0].lower() in metrics_df.columns: # identify cells that fail criterion:\n",
    "            tmp_rej = []\n",
    "            tmp_rej.append(metrics_df.loc[metrics_df[i[0].lower()] <= float(i[2])].index)\n",
    "            #print('Checking cells {}...'.format(i[0]))\n",
    "            cell_rej += tmp_rej\n",
    "        else: \n",
    "            print('{} is not a valid metric.'.format(i[0]))\n",
    "\n",
    "    os.remove(metric_fn)\n",
    "    \n",
    "    out_list = sorted(list(set([i for j in cell_rej for i in j])))\n",
    "    accept_list = [i for i in range(cellset.num_cells) if i not in out_list]\n",
    "    \n",
    "    # modify cellsets:\n",
    "    for i in out_list:\n",
    "        cellset.set_cell_status(i,'rejected')\n",
    "    for i in accept_list:\n",
    "        cellset.set_cell_status(i,'accepted')    \n",
    "    \n",
    "    \n",
    "    return out_list\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellset_data_volumes(cs_file, snr_thresh=2, isx_ed_threshold = 5, autosort_snr = 5):\n",
    "    #\n",
    "    # reads cellset specified by cs_file and returns:\n",
    "    # [trace_mtx, raster_mtx, raster_mad_mtx]\n",
    "    #\n",
    "    # trace_mtx is [n,m] matrix of m-cells n-sample calcium signals \n",
    "    # raster_mtx is [n,m] matrix of m-cells n-sample calcium events (0-1) deteted with event_detection_cnmfe()\n",
    "    # raster_mas_mtx is [n,m] matrix of m-cells n-sample calcium events (0-1) deteted with isx.event_detection()\n",
    "    # \n",
    "    # returns accepted/undecided cells only\n",
    "    #\n",
    "    \n",
    "    # read cellset:\n",
    "    cellset = isx.CellSet.read(cs_file)\n",
    "\n",
    "    # event detection:\n",
    "    ed_file = isx.make_output_file_path(cs_file, os.path.dirname(cs_file), 'ED')\n",
    "    if os.path.isfile(ed_file):\n",
    "        os.remove(ed_file)\n",
    "    event_detection_cnmfe(cellset, snr_thresh = snr_thresh, peak_dist = 0.3, peak_width = 0.2)\n",
    "\n",
    "    # isx (MAD) event detection:\n",
    "    isxed_file = isx.make_output_file_path(cs_file, os.path.dirname(cs_file), 'isxED')\n",
    "    if os.path.isfile(isxed_file):\n",
    "        os.remove(isxed_file)\n",
    "    isx.event_detection(cs_file, isxed_file, event_time_ref='maximum', threshold=isx_ed_threshold)\n",
    "\n",
    "    \n",
    "    # build trace, raster, peak data volumes:\n",
    "    trace_volume = build_trace_volume([cs_file])\n",
    "    raster_volume = build_raster_volume([isx.make_output_file_path(cs_file, os.path.dirname(cs_file), 'ED')], sm_win=0)\n",
    "    raster_isx_volume = build_raster_volume([isx.make_output_file_path(cs_file, os.path.dirname(cs_file), 'isxED')], sm_win=0)\n",
    "    #peak_volume = (raster_volume*trace_volume) # rasters with scalar event sizes\n",
    "    #peak_isx_volume = (raster_isx_volume*trace_volume)\n",
    "    #ref_volume = np.copy(trace_volume)\n",
    "    #ref_volume[10:,0] = signal.medfilt(ref_volume[:-10,0],7)\n",
    "\n",
    "    # get length of recording:\n",
    "    #max_t = (isx.Duration.to_usecs(cellset.timing.period)*1e-6) * cellset.timing.num_samples\n",
    "    \n",
    "    # auto accept/reject:\n",
    "    isx.auto_accept_reject(cs_file, isxed_file, filters = [('# Comps', '=', 1), ('Event Rate', '>', 0), ('SNR', '>', autosort_snr)])\n",
    "    \n",
    "\n",
    "    # create mask for excluding rejected cells:\n",
    "    cell_mask = np.ones((cellset.num_cells))\n",
    "    #print(cellset.num_cells)\n",
    "    for i in range(cellset.num_cells):\n",
    "        if cellset.get_cell_status(i) == 'rejected':\n",
    "            cell_mask[i] = 0\n",
    "\n",
    "    accepted_cells = np.nonzero(cell_mask)[0]\n",
    "    \n",
    "    return [trace_volume[0][:,accepted_cells], raster_volume[0][:,accepted_cells], raster_isx_volume[0][:,accepted_cells]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cellset_paths(df):\n",
    "    '''\n",
    "    get_cellset_paths(df, col_dict = {})\n",
    "    \n",
    "    INPUTS:\n",
    "    df <pandas dataframe>: must have these columns: 'data_dir_ca', 'isxd_data_basename'\n",
    "    \n",
    "    OUTPUTS:\n",
    "    out_list <list>: list of isxd cellsets meeting conditions in col_dict\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    for pathname,filename in zip(df.data_dir_ca.values, df.isxd_data_basename.values):\n",
    "        pathname = fix_data_path(pathname)\n",
    "        if os.path.isdir(pathname):\n",
    "            if len([i for i in os.listdir(pathname) if 'pcaica.isxd' in i]): # check base directory for pcaica.isxd cellset\n",
    "                cellset_fn = pathname + [i for i in os.listdir(pathname) if 'pcaica.isxd' in i][0]\n",
    "                out_list.append(cellset_fn)\n",
    "            else: # check for pipeline directory\n",
    "                if len([i for i in os.listdir(pathname) if 'pipeline' in i]):\n",
    "                    pipeline_path = [i for i in os.listdir(pathname) if 'pipeline' in i][0]\n",
    "                    if len([i for i in os.listdir(pathname+pipeline_path) if 'pcaica.isxd' in i]):\n",
    "                        cellset_fn = pathname + pipeline_path + '/' + [i for i in os.listdir(pathname+pipeline_path) if 'pcaica.isxd' in i][0]\n",
    "                        out_list.append(cellset_fn)\n",
    "\n",
    "\n",
    "    \n",
    "    #print([fix_data_path(i) for i in df.data_dir_ca],'\\n')\n",
    "    #print([i for i in df.isxd_data_basename],'\\n')\n",
    "    \n",
    "    return out_list\n",
    "\n",
    "    \n",
    "\n",
    "def get_cellset_timescale(cellset_fn):\n",
    "    #\n",
    "    # returns time vector for traces in cellset_fn\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    cs = isx.CellSet.read(cellset_fn)\n",
    "    per = cs.timing.period.to_usecs()*1e-6\n",
    "    \n",
    "    tvect = np.arange(0,cs.timing.num_samples * per, per)\n",
    "    return tvect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data_path(the_path, base_path = 'ariel', path_sep = '\\\\'):\n",
    "    '''\n",
    "    fix_data_path(thepath, basepath = '/ariel/'):\n",
    "    \n",
    "    INPUTS:\n",
    "    the_path <str>:\n",
    "    base_path <str>:\n",
    "    \n",
    "    OUTPUTS:\n",
    "    out_path <str>:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    path_elements = the_path.split(path_sep)\n",
    "    for idx,i in enumerate(path_elements):  \n",
    "        if i=='science-1':\n",
    "            path_elements[idx] = 'science'\n",
    "        if i=='data':\n",
    "            path_elements[idx] = ''\n",
    "    path_elements = [base_path] + [i for i in path_elements if len(i)]\n",
    "    path_elements = ['/']+[i+'/' for i in path_elements]\n",
    "\n",
    "    return ''.join(path_elements)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frame_lookup(gpio_csv_fn, cellset_fn, sync_chan_name = ' GPIO-1', offset = 0, pulse_thresh =0.55):\n",
    "    '''\n",
    "    Creates lookup table between TTL pulses on gpio input channel and data samples in a cellset. Output is on cellset timebase so that\n",
    "    frame_lookup[10] = behavioral video frame corresponding to cellset isxd sample 10. \n",
    "    \n",
    "    INPUTS:\n",
    "    gpio_csv_fn <string> : filename of gpio .csv file, created via IDPS/isx export of gpio.isxd file\n",
    "    cellset_fn <string> : filename of cellset.isxd file\n",
    "    sync_chan_name <string = 'GPIO-1'> : GPIO channel that recorded TTL sync pulses\n",
    "    offset <int = 0>\n",
    "    pulse_thresh <float = 0.55> : threshold (0-1) for detecting normalized pulses (1 = max pulse amplitude)\n",
    "    \n",
    "    OUTPUT:\n",
    "    frame_lookup <np.array> : lookup table on cellset timebase for behavioral video frames, i.e. frame_lookup[10] = behavioral frame corresponding to 10th\n",
    "                                cellset data sample.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cs_time = (get_cellset_timescale(cellset_fn))\n",
    "    fs = round(1/cs_time[1], 8)\n",
    "\n",
    "    if os.path.isfile(gpio_fn):\n",
    "        gpio = pd.read_csv(gpio_fn)\n",
    "        sync_df = gpio.loc[gpio[' Channel Name'] == sync_chan_name]\n",
    "        sync_df.reset_index(inplace=True, drop=True)\n",
    "        sync_sub = sync_df[['Time (s)',' Value']]\n",
    "    else:\n",
    "        print('{} file does not exist'.format(gpio_fn))\n",
    "        \n",
    "    \n",
    "    # detect pulse onsets and offsets in gpio:\n",
    "    in_data = sync_df[' Value'].astype(np.float32)\n",
    "    in_time = sync_df['Time (s)'].astype(np.float32)\n",
    "\n",
    "    crossings = np.diff(1 * (in_data >  max(in_data) * pulse_thresh) != 0 )\n",
    "\n",
    "    pulse_onsets = in_time[1:][crossings].values[0::2]\n",
    "    pulse_offsets = in_time[1:][crossings].values[1::2]\n",
    "\n",
    "    start_t = pulse_onsets[0] - offset\n",
    "    stop_t = pulse_onsets[-1] + (1/fs) - offset\n",
    "    \n",
    "    # create behavioral video frame lookup table and map to isxd timebase:\n",
    "    sync_times = np.zeros((len(cs_time), 1))\n",
    "    ons_idx = ((pulse_onsets - offset) * fs).astype(int)\n",
    "    sync_times[ons_idx] = 1\n",
    "    frame_lookup = np.cumsum(sync_times).astype(int)\n",
    "    frame_lookup[np.argmax(frame_lookup)+1:] = 0\n",
    "    \n",
    "    return frame_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_com_entries(com, x_threshs, entry_dir = (0 , 1), t_thresh = 10, detect_win = 5, extract_win = 10):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    com <n x 2 array>\n",
    "    x_threshs <n=2 array>\n",
    "    entry_dir = (0 , 1)<n=2 array>\n",
    "    t_thresh = 10 (int, frames)\n",
    "    detect_win = 5 (int, pixels)\n",
    "    extract_win = 10 (int, frames)\n",
    "\n",
    "    OUTPUTS:\n",
    "    outdict <dict> : dicttionary with keys: x_threshs, outdict[x_thresh].keys() = [entries, entry frames, exits, exit frames]\n",
    "    \n",
    "    '''\n",
    "\n",
    "    outdict = dict()\n",
    "    for t in x_threshs:\n",
    "        outdict[t] = {}\n",
    "        outdict[t]['entries'] = []\n",
    "        outdict[t]['entry frames'] = []\n",
    "        outdict[t]['exits'] = []\n",
    "        outdict[t]['exit frames'] = []\n",
    "    \n",
    "    for t, cross_dir in zip(x_threshs, entry_dir):\n",
    "\n",
    "        thresh_range  = np.arange(t - detect_win, t + detect_win)\n",
    "        crosspnts = np.intersect1d(np.argwhere(com[:,1] > thresh_range[0]), np.argwhere(com[:,1] < thresh_range[-1])  )            \n",
    "        crosspnts = crosspnts[np.diff(crosspnts, prepend = [0]) > t_thresh]\n",
    "        \n",
    "        crossings = []\n",
    "        \n",
    "        for p in crosspnts:\n",
    "            if (p+extract_win < np.shape(com)[0]) and (p-extract_win > 0): # if extraction windows do not extend past length of data, extract COM segment:\n",
    "                the_seg = com[p - extract_win : p + extract_win,:]\n",
    "                crossings.append(the_seg)\n",
    "        if len(crossings):\n",
    "            crossings = np.asarray(crossings).astype(int)\n",
    "        else:\n",
    "            crossings = np.asarray([])\n",
    "\n",
    "        # distinguish entries from exits:\n",
    "        pop_idx = []\n",
    "        keep_idx = []\n",
    "\n",
    "        for idx, i in enumerate(crossings):\n",
    "            if cross_dir == 0: # right to left entries\n",
    "                if i[0,1] < i[-1,1]:\n",
    "                    pop_idx.append(idx)\n",
    "                else:\n",
    "                    keep_idx.append(idx)\n",
    "            elif cross_dir == 1: # left to right entries\n",
    "                if i[0,1] > i[-1,1]:\n",
    "                    pop_idx.append(idx)\n",
    "                else:\n",
    "                    keep_idx.append(idx)\n",
    "                \n",
    "                \n",
    "        entries = crossings[keep_idx]\n",
    "        exits = crossings[pop_idx]\n",
    "        \n",
    "        outdict[t]['entries'] = entries\n",
    "        outdict[t]['entry frames'] = crosspnts[keep_idx]\n",
    "        outdict[t]['exits'] = exits\n",
    "        outdict[t]['exit frames'] = crosspnts[pop_idx]\n",
    "        \n",
    "    return outdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splice_and_align(data_mtx, align_pnts, win = (20,20)):\n",
    "    '''\n",
    "    Extracts data in data_mtx around samples in align_pnts and returns aligned data matrix +/- window\n",
    "    \n",
    "    INPUTS:\n",
    "    data_mtx <float array> : samples x signals (e.g. cells) matrix\n",
    "    align_pnts <int vector> : alignment points (samples) from dim=0 of data_mtx\n",
    "    win <2-tuple> = (20,20) : pre and post extraction windows around each alignment point\n",
    "    \n",
    "    OUTPUTS:\n",
    "    out_mtx <float array> : data_mtx.shape[1] x (win(0)+win(1)) array of data_mtx aligned to alignment points\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    out_mtx = np.zeros((data_mtx.shape[1], len(align_pnts), win[0]+win[1]))\n",
    "\n",
    "    for sidx,trace in enumerate(data_mtx.transpose()): # each cell\n",
    "        for aidx,pnt in enumerate(align_pnts): # each alignment point:\n",
    "            if (pnt - win[0] >=0) and (pnt + win[1] <= len(trace)):\n",
    "                out_mtx[sidx, aidx, :] = trace[pnt-win[0]:pnt+win[1] ]\n",
    "\n",
    "    \n",
    "    return out_mtx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_aligned_trace_dict(stat_dict, framewin, data_type = 'trace_mtx', x_thresh = (95,230), win = (20,60), event_types = ['entry frames'], shuffle = False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #framewin = (phase_start, phase_stop) # segment of video frames to analyze\n",
    "\n",
    "    aligned_traces_dict =  dict()\n",
    "\n",
    "    for cond in stat_dict.keys():\n",
    "        #print(cond)\n",
    "        datasets = stat_dict[cond].keys()\n",
    "        aligned_traces_dict[cond] = {}\n",
    "        for ds in stat_dict[cond]:\n",
    "            #print('\\t',ds)\n",
    "            the_com = stat_dict[cond][ds]['COM'][framewin[0] : framewin[1], :]\n",
    "            align_dict = find_com_entries(the_com, x_thresh)\n",
    "            aligned_traces_dict[cond][ds] = {}\n",
    "            for align_point in x_thresh:\n",
    "                aligned_traces_dict[cond][ds][align_point] = {}\n",
    "                for the_event_type in event_types:\n",
    "                    aligned_traces_dict[cond][ds][align_point][the_event_type] = {}\n",
    "                    if shuffle == True:\n",
    "                        #print('Using random indices')\n",
    "                        randvect = np.random.choice(np.arange(framewin[0], framewin[1]), size = len(align_dict[align_point][the_event_type]), replace=False )\n",
    "                        align_indices = [np.arange(i-win[0], i+win[1]) for i in randvect]\n",
    "                    else:\n",
    "                        #print('Aligning to triggers')\n",
    "                        align_indices = [np.arange(i-win[0], i+win[1]) for i in align_dict[align_point][the_event_type]]\n",
    "                    avg_mtx = np.mean(stat_dict[cond][ds][data_type][align_indices, :], axis = 0)\n",
    "                    all_trial_mtx = stat_dict[cond][ds][data_type][align_indices, :]\n",
    "                    aligned_traces_dict[cond][ds][align_point][the_event_type]['aligned_mtx'] = all_trial_mtx\n",
    "                    \n",
    "        \n",
    "    return aligned_traces_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronization analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_variance_stabilization(invect):\n",
    "    '''\n",
    "    returns variance-stabilized standard deviation, after Wang et al (2019) doi:10.1038/s41593-019-0492-2\n",
    "    \n",
    "    INPUTS:\n",
    "    invect <1d array of ints or floats>\n",
    "    \n",
    "    OUTPUTS:\n",
    "    outval <float> variance stablized standard deviation of invect\n",
    "    '''\n",
    "    \n",
    "    out_val = np.sqrt(np.mean(np.diff(invect, prepend = invect[0])**2) / 2)\n",
    "    \n",
    "    return out_val\n",
    "    \n",
    "def measure_active_fraction(trace_mtx, baseline_range = [], thresh_coef = 1, rate_thresh_coef= 5, rate_thresh = 0.01):\n",
    "    \"\"\"\n",
    "    Measures the fraction of elements in trace_mtx that exceed a threshold, for each sample in trace_mtx\n",
    "    \n",
    "    trace_mtx <n cells X m samples array>\n",
    "    baseline_range <tuple of ints> start and stop points of window for computing the threshold\n",
    "    thresh_coef <float > 0> values in each row of trace_mtx < (thres_coef * thresh(row)) are set to 'inactive', values > (thres_coef * thresh(row)) are active\n",
    "    rate_thresh_coef <float > 0> Used in conjunction with rate_thresh to determine if signal has sufficient events to detect. \n",
    "        It is a multiple of thresh that needs to be exceeded for signals to be included. \n",
    "    rate_thresh <0 < float < 1> Used in conjunction with rate_thresh_coef to determine if threshold crossings for a signal/row should be included in \n",
    "        fraction active. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #trace_mtx = (trace_mtx - np.min(trace_mtx)) / (np.max(trace_mtx) - np.min(trace_mtx))\n",
    "    trace_out_mtx = copy.deepcopy(trace_mtx)\n",
    "    \n",
    "    thresh_mtx = np.zeros_like(trace_mtx)\n",
    "    active_mtx = np.zeros_like(trace_mtx)\n",
    "    \n",
    "    if len(baseline_range) == 0:\n",
    "        baseline_range = (0, len(trace_mtx))\n",
    "    \n",
    "    for idx, the_trace in enumerate(trace_mtx):\n",
    "        the_thresh = noise_variance_stabilization(abs(the_trace[baseline_range[0]:baseline_range[1]]))\n",
    "        #print(len(the_trace), the_thresh)\n",
    "        thresh_trace = np.copy(the_trace)\n",
    "        if len(np.argwhere(thresh_trace > (the_thresh * rate_thresh_coef)).flatten()) > (rate_thresh * len(the_trace)):\n",
    "            thresh_trace[thresh_trace < (the_thresh * thresh_coef)] = np.nan\n",
    "            active_trace = 1*(the_trace >= (the_thresh * thresh_coef))            \n",
    "        else:\n",
    "            thresh_trace[:] = np.nan\n",
    "            active_trace = np.zeros_like(the_trace)\n",
    "        thresh_mtx[idx,:] = thresh_trace\n",
    "        active_mtx[idx,:] = active_trace\n",
    "        \n",
    "    active_fraction = np.sum(active_mtx, axis=0) / active_mtx.shape[0]\n",
    "        \n",
    "    return [active_fraction, {'traces': trace_out_mtx,'thresh_traces':thresh_mtx, 'active_raster':active_mtx}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_from_dist(test_val, comp_dist, hist_range = (0,1), tails = 2):\n",
    "    '''\n",
    "    Computes probability of observing test_val given distribution comp_dist\n",
    "    \n",
    "    INPUTS:\n",
    "    test_val <float> : observed value\n",
    "    comp_dist <float array> : distribution to test test_val against\n",
    "    hist_range <float tuple> : range of probability distribution to create, typially exceeds (min(comp_dist), max(comp_dist))\n",
    "    tails <int = (1,2)> : set to 1 to for one-tailed test (probability of observing a value less than test_val) or 2 for two-tailed test \n",
    "                            (probability of observing more extreme high or low value than test_val)\n",
    "                            \n",
    "    OUTPUTS:\n",
    "    < 0 >= float >= 1 > : one or two-tailed posterior probability of observing test_val value given comp_dist distribution \n",
    "    \n",
    "    '''\n",
    "    #max_val = max((test_val, max(comp_dist)))\n",
    "    #min_val = min((test_val, min(comp_dist)))\n",
    "    h,hbins = np.histogram(comp_dist, bins = 100, range = hist_range)\n",
    "    hdist = stats.rv_histogram((h, hbins) )\n",
    "    #modulation_p_dist.append(min((hdist.cdf(np.median(cell_stat)), 1-hdist.cdf(np.median(cell_stat)))) )\n",
    "    if tails == 2:\n",
    "        return min((hdist.cdf(test_val), 1-hdist.cdf(test_val))) \n",
    "    elif tails == 1:\n",
    "        return hdist.sf(test_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = pickle.load(open('frame_aligned_data_20200415.pkl' , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stat_dict.keys():\n",
    "    print(i)\n",
    "    for j in stat_dict[i].keys():\n",
    "        print('\\t',j)\n",
    "        for k in stat_dict[i][j].keys():\n",
    "            print('\\t\\t',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dir = '/home/mmiller/Documents/data/astellas/fmr1ko_linear_social/behavior_annotations/'\n",
    "annotations_list = [i for i in os.listdir(annotations_dir) if '.csv' in i]\n",
    "print(annotations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sides_dict = pickle.load(open('target_sides_20200415.pkl','rb') )\n",
    "sides_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap annotations based on social target side (R/L):\n",
    "### Use COM to distinguish social/object and familiar/novel nose-to-other events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_annotations.dtypes,'\\n')\n",
    "print(df_annotations.label.unique(),'\\n')\n",
    "df_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "sides = (95,230)\n",
    "the_cond = 'FMR1CTRL'\n",
    "the_subj = 'OM051'\n",
    "\n",
    "# load annotations:\n",
    "df_annotations = pd.read_csv([annotations_dir+i for i in annotations_list if the_subj in i][0], index_col=0)\n",
    "\n",
    "# load center-of-mass. This is already rotated so left side is social side:\n",
    "the_com = stat_dict[the_cond][the_subj]['COM']\n",
    "\n",
    "# remap social events to left = social / right = object frame of reference:\n",
    "if sides_dict[the_subj] == 'r':\n",
    "    df_annotations.label.loc[df_annotations.label == 'approach right side'] = 'approach left'\n",
    "    df_annotations.label.loc[df_annotations.label == 'approach left side'] = 'approach right'\n",
    "elif sides_dict[the_subj] == 'l':\n",
    "    df_annotations.label.loc[df_annotations.label == 'approach right side'] = 'approach right'\n",
    "    df_annotations.label.loc[df_annotations.label == 'approach left side'] = 'approach left'\n",
    "    \n",
    "# get social / non-social COM frames:\n",
    "social_frames = np.argwhere(the_com[:,1] <= sides[0]).flatten()\n",
    "object_frames = np.argwhere(the_com[:,1] >= sides[1]).flatten()\n",
    "\n",
    "# remap social / non-social 'nose-to-other' and 'rearing on cup' events based on social target side:\n",
    "df_annotations.label.loc[(df_annotations.frame.isin(object_frames)) & (df_annotations.label == 'nose-to-other')] = 'nose-to-right'\n",
    "df_annotations.label.loc[(df_annotations.frame.isin(social_frames)) & (df_annotations.label == 'nose-to-other')] = 'nose-to-left'\n",
    "df_annotations.label.loc[(df_annotations.frame.isin(object_frames)) & (df_annotations.label == 'rearing on cup')] = 'rearing right'\n",
    "df_annotations.label.loc[(df_annotations.frame.isin(social_frames)) & (df_annotations.label == 'rearing on cup')] = 'rearing left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add labels field to stat_dict with no repeated measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "sides = (95,230)\n",
    "\n",
    "for the_cond in stat_dict.keys():\n",
    "    print(the_cond)\n",
    "    for the_subj in stat_dict[the_cond]:\n",
    "        print('\\t',the_subj)\n",
    "        # load annotations:\n",
    "        df_annotations = pd.read_csv([annotations_dir+i for i in annotations_list if the_subj in i][0], index_col=0)\n",
    "\n",
    "        # load center-of-mass. This is already rotated so left side is social side:\n",
    "        the_com = stat_dict[the_cond][the_subj]['COM']\n",
    "\n",
    "        # remap social events to left = social / right = object frame of reference:\n",
    "        if sides_dict[the_subj] == 'r':\n",
    "            df_annotations.label.loc[df_annotations.label == 'approach right side'] = 'approach left'\n",
    "            df_annotations.label.loc[df_annotations.label == 'approach left side'] = 'approach right'\n",
    "        elif sides_dict[the_subj] == 'l':\n",
    "            df_annotations.label.loc[df_annotations.label == 'approach right side'] = 'approach right'\n",
    "            df_annotations.label.loc[df_annotations.label == 'approach left side'] = 'approach left'\n",
    "\n",
    "        # get social / non-social COM frames:\n",
    "        social_frames = np.argwhere(the_com[:,1] <= sides[0]).flatten()\n",
    "        object_frames = np.argwhere(the_com[:,1] >= sides[1]).flatten()\n",
    "\n",
    "        # remap social / non-social 'nose-to-other' and 'rearing on cup' events based on social target side:\n",
    "        df_annotations.label.loc[(df_annotations.frame.isin(object_frames)) & (df_annotations.label == 'nose-to-other')] = 'nose-to-right'\n",
    "        df_annotations.label.loc[(df_annotations.frame.isin(social_frames)) & (df_annotations.label == 'nose-to-other')] = 'nose-to-left'\n",
    "        df_annotations.label.loc[(df_annotations.frame.isin(object_frames)) & (df_annotations.label == 'rearing on cup')] = 'rearing right'\n",
    "        df_annotations.label.loc[(df_annotations.frame.isin(social_frames)) & (df_annotations.label == 'rearing on cup')] = 'rearing left'\n",
    "        \n",
    "        stat_dict[the_cond][the_subj]['labels'] = df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict[the_cond][the_subj]['labels'] = df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(stat_dict['FMR1CTRL']['OM051']['labels'].label.values == 'nose-to-ag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'frame_aligned_data_20200415_labels.pkl'\n",
    "with open(fn, 'wb') as fp:\n",
    "    pickle.dump(stat_dict,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add labels field to stat_dict with repeated measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = pickle.load(open('frame_aligned_data_baclofen.pkl' , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMR1CTRL\n",
      "\t linear_social_saline\n",
      "\t\t OM047\n",
      "\t\t OM035\n",
      "\t\t OM051\n",
      "\t linear_social_1.5mgkg_rBaclofen\n",
      "\t\t OM051\n",
      "\t\t OM035\n",
      "\t\t OM047\n",
      "FMR1KO\n",
      "\t linear_social_saline\n",
      "\t\t OM045\n",
      "\t\t OM038\n",
      "\t\t OM042\n",
      "\t\t OM040\n",
      "\t linear_social_1.5mgkg_rBaclofen\n",
      "\t\t OM042\n",
      "\t\t OM040\n",
      "\t\t OM045\n",
      "\t\t OM038\n"
     ]
    }
   ],
   "source": [
    "for i in stat_dict.keys():\n",
    "    print(i)\n",
    "    for j in stat_dict[i].keys():\n",
    "        print('\\t',j)\n",
    "        for k in stat_dict[i][j].keys():\n",
    "            print('\\t\\t',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dir = '/ariel/science/OM/Astellas/social_linear_behavior/April2020_FMR1_LinearSocial/annotations/'\n",
    "annotations_list = [i for i in os.listdir(annotations_dir) if '.csv' in i]\n",
    "print(annotations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sides_dict = pickle.load(open('target_sides_baclofen.pkl','rb') )\n",
    "sides_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "sides = (95,230)\n",
    "\n",
    "for the_cond in stat_dict.keys():\n",
    "    print(the_cond)\n",
    "    for the_exp in stat_dict[the_cond].keys(): # each repeated measure:\n",
    "        for the_subj in stat_dict[the_cond]:\n",
    "            print('\\t',the_subj)\n",
    "            # load annotations:\n",
    "            df_annotations = pd.read_csv([annotations_dir+i for i in annotations_list if the_subj in i][0], index_col=0)\n",
    "\n",
    "            # load center-of-mass. This is already rotated so left side is social side:\n",
    "            the_com = stat_dict[the_cond][the_subj]['COM']\n",
    "\n",
    "            # remap social events to left = social / right = object frame of reference:\n",
    "            if sides_dict[the_subj] == 'r':\n",
    "                df_annotations.label.loc[df_annotations.label == 'approach right side'] = 'approach left'\n",
    "                df_annotations.label.loc[df_annotations.label == 'approach left side'] = 'approach right'\n",
    "            elif sides_dict[the_subj] == 'l':\n",
    "                df_annotations.label.loc[df_annotations.label == 'approach right side'] = 'approach right'\n",
    "                df_annotations.label.loc[df_annotations.label == 'approach left side'] = 'approach left'\n",
    "\n",
    "            # get social / non-social COM frames:\n",
    "            social_frames = np.argwhere(the_com[:,1] <= sides[0]).flatten()\n",
    "            object_frames = np.argwhere(the_com[:,1] >= sides[1]).flatten()\n",
    "\n",
    "            # remap social / non-social 'nose-to-other' and 'rearing on cup' events based on social target side:\n",
    "            df_annotations.label.loc[(df_annotations.frame.isin(object_frames)) & (df_annotations.label == 'nose-to-other')] = 'nose-to-right'\n",
    "            df_annotations.label.loc[(df_annotations.frame.isin(social_frames)) & (df_annotations.label == 'nose-to-other')] = 'nose-to-left'\n",
    "            df_annotations.label.loc[(df_annotations.frame.isin(object_frames)) & (df_annotations.label == 'rearing on cup')] = 'rearing right'\n",
    "            df_annotations.label.loc[(df_annotations.frame.isin(social_frames)) & (df_annotations.label == 'rearing on cup')] = 'rearing left'\n",
    "\n",
    "            stat_dict[the_cond][the_subj]['labels'] = df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict[the_cond][the_subj]['labels'] = df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(stat_dict['FMR1CTRL']['OM051']['labels'].label.values == 'nose-to-ag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'frame_aligned_data_20200415_labels.pkl'\n",
    "with open(fn, 'wb') as fp:\n",
    "    pickle.dump(stat_dict,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### analyze events by genotype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdict = dict(zip(stat_dict.keys(), [plt.get_cmap('tab10')(i) for i in range(len(stat_dict.keys()))]))\n",
    "cdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phase_dict = {'habituate':(500,5500), 'preference':(6500,6500+11500), 'novelty':(18500,18500+11500)}\n",
    "phase_dict = {'preference':(6500,6500+11500)}\n",
    "behaviors = ( ('approach left', 'approach right'), ('rearing left', 'rearing right'), ('nose-to-left', 'nose-to-right'),\n",
    "            ('nose-to-nose', 'nose-to-ag'), ('boxing', 'grooming')\n",
    "            )\n",
    "plot_jog = .5\n",
    "phase_jog = plot_jog * 1.5\n",
    "cond_jog = phase_jog * len(phase_dict)\n",
    "beh_jog = cond_jog * 3\n",
    "\n",
    "x_ticks = []\n",
    "\n",
    "f,ax = plt.subplots(1,1,figsize=(20,6))\n",
    "\n",
    "for beh_i, the_behaviors in enumerate(behaviors):\n",
    "    #print(the_behaviors)\n",
    "    for cond_i, the_cond in enumerate(stat_dict.keys()):\n",
    "        #print(the_cond)\n",
    "        for phase_i, the_phase in enumerate(phase_dict.keys()):\n",
    "            x1 = (cond_i * cond_jog) + (phase_i * phase_jog) + (beh_i * beh_jog)\n",
    "            #print(x1, x1+plot_jog)\n",
    "            for the_subj in stat_dict[the_cond].keys():\n",
    "                df_labels = stat_dict[the_cond][the_subj]['labels']\n",
    "                ax.plot( (x1, x1 + plot_jog), \n",
    "                        (sum(df_labels.label.loc[df_labels.frame.isin(np.arange(phase_dict[the_phase][0],phase_dict[the_phase][1]))].values == the_behaviors[0])/20, \n",
    "                         sum(df_labels.label.loc[df_labels.frame.isin(np.arange(phase_dict[the_phase][0],phase_dict[the_phase][1]))].values == the_behaviors[1])/20), \n",
    "                         'o-', color = cdict[the_cond], markersize=10, alpha=1)\n",
    "            #x_ticks.append( (x1, x1 + plot_jog) )\n",
    "    \n",
    "    x_ticks.append((cond_i * cond_jog) + (phase_i * phase_jog) + (beh_i * beh_jog) - (plot_jog/4) )\n",
    "\n",
    "\n",
    "twoaxis(ax)\n",
    "ax.set_xticks(np.asarray(x_ticks).flatten())\n",
    "ax.set_xticklabels(behaviors, fontsize=14, rotation=345, horizontalalignment='left' )\n",
    "ax.set_ylabel('time spent (s)', fontsize=14)\n",
    "ax.set_ylim(0,100)\n",
    "ax.grid(color='gray', alpha=.5, axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### auROC ID of tuned populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sandbox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_cond = 'FMR1KO'\n",
    "the_subj = 'OM038'\n",
    "event_types = ['nose-to-left', 'nose-to-nose', 'nose-to-ag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = stat_dict[the_cond][the_subj]['labels']\n",
    "align_pnts = df_labels.frame.loc[df_labels.label.isin(event_types)].values\n",
    "trace_mtx = stat_dict[the_cond][the_subj]['trace_mtx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert alignment points to raster:\n",
    "align_raster = np.zeros((trace_mtx.shape[0],))\n",
    "align_raster[align_pnts] = 1\n",
    "print(sum(align_raster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cells with significant auROC based on time-shuffled distribution:\n",
    "frames = (6200,6200+11500)\n",
    "n_iter = 500\n",
    "\n",
    "auroc = []\n",
    "auroc_p = []\n",
    "\n",
    "for cellnum,the_trace in enumerate(trace_mtx[frames[0]:frames[1],:].transpose()):\n",
    "    the_auroc = metrics.roc_auc_score(align_raster[frames[0]:frames[1]], the_trace)\n",
    "    auroc.append(the_auroc)\n",
    "    shift_auroc = []\n",
    "    for the_iter in range(n_iter):\n",
    "        shift_trace = np.roll(the_trace, np.random.choice(np.arange(len(the_trace))))\n",
    "        shift_auroc.append(metrics.roc_auc_score(align_raster[frames[0]:frames[1]], shift_trace) )\n",
    "    auroc_p.append(prob_from_dist(the_auroc, shift_auroc, hist_range = (-1,1), tails = 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_cells = np.argwhere(np.asarray(auroc_p) <= .01).flatten()\n",
    "pos_mod_cells = np.intersect1d(mod_cells, np.argwhere(np.asarray(auroc) >= .6).flatten())\n",
    "neg_mod_cells = np.intersect1d(mod_cells, np.argwhere(np.asarray(auroc) <= .4).flatten())\n",
    "print(pos_mod_cells)\n",
    "print(len(pos_mod_cells) / len(auroc))\n",
    "print(neg_mod_cells)\n",
    "print(len(neg_mod_cells) / len(auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellnum = np.argmax(auroc)\n",
    "print(cellnum)\n",
    "\n",
    "f,ax = plt.subplots(1,1,figsize=(20,4))\n",
    "ax.plot(signal.medfilt(trace_mtx[phase_dict['preference'][0]:phase_dict['preference'][1],cellnum], 15))\n",
    "#ax.plot(np.mean(trace_mtx[phase_dict['preference'][0]:phase_dict['preference'][1],negpos_mod_cellsd_cells], axis=1))\n",
    "ax.plot(align_raster[phase_dict['preference'][0]:phase_dict['preference'][1]]*max(trace_mtx[phase_dict['preference'][0]:phase_dict['preference'][1],cellnum]), alpha=.75)\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "f,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "\n",
    "title_str = ''\n",
    "for cellnum in pos_mod_cells:\n",
    "    fpr, tpr, roc_thresh = metrics.roc_curve(align_raster[frames[0]:frames[1]], trace_mtx[frames[0]:frames[1], cellnum])\n",
    "    auc = metrics.roc_auc_score(align_raster[frames[0]:frames[1]], trace_mtx[frames[0]:frames[1], cellnum])\n",
    "    ax[0].plot(fpr,tpr, color=cmap(0), alpha=.5)\n",
    "    \n",
    "title_str += str(round(np.mean([auroc[i] for i in pos_mod_cells]),2))\n",
    "fpr, tpr, roc_thresh = metrics.roc_curve(align_raster[frames[0]:frames[1]], np.mean(trace_mtx[frames[0]:frames[1], pos_mod_cells], axis=1))\n",
    "auc = metrics.roc_auc_score(align_raster[frames[0]:frames[1]], np.mean(trace_mtx[frames[0]:frames[1], pos_mod_cells], axis=1))\n",
    "ax[0].plot(fpr,tpr, color=cmap(0), alpha=1, linewidth=1.5)\n",
    "title_str += ', '+str(round(auc,2))\n",
    "ax[0].set_title(title_str)\n",
    "\n",
    "title_str = ''\n",
    "for cellnum in neg_mod_cells:\n",
    "    fpr, tpr, roc_thresh = metrics.roc_curve(align_raster[frames[0]:frames[1]], trace_mtx[frames[0]:frames[1], cellnum])\n",
    "    auc = metrics.roc_auc_score(align_raster[frames[0]:frames[1]], trace_mtx[frames[0]:frames[1], cellnum])\n",
    "    ax[1].plot(fpr,tpr, color=cmap(0), alpha=.5)\n",
    "\n",
    "title_str += str(round(np.mean([auroc[i] for i in neg_mod_cells]),2))\n",
    "fpr, tpr, roc_thresh = metrics.roc_curve(align_raster[frames[0]:frames[1]], np.mean(trace_mtx[frames[0]:frames[1], neg_mod_cells], axis=1))\n",
    "auc = metrics.roc_auc_score(align_raster[frames[0]:frames[1]], np.mean(trace_mtx[frames[0]:frames[1], neg_mod_cells], axis=1))\n",
    "ax[1].plot(fpr,tpr, color=cmap(0), alpha=1, linewidth=1.5)\n",
    "title_str += ', '+str(round(auc,2))\n",
    "ax[1].set_title(title_str)\n",
    "\n",
    "for a in ax:\n",
    "    twoaxis(a)\n",
    "    a.grid('on', color='gray', alpha=.5)\n",
    "    a.plot([0,1], [0,1], color='gray', alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot spatial map of events:\n",
    "f,ax = plt.subplots(1,1,figsize=(10,4))\n",
    "ax.plot(stat_dict[the_cond][the_subj]['COM'][6000:18000,1], stat_dict[the_cond][the_subj]['COM'][6000:18000,0], 'o-', markersize=1, alpha=.25, color='gray')\n",
    "ax.plot(stat_dict[the_cond][the_subj]['COM'][align_pnts,1], stat_dict[the_cond][the_subj]['COM'][align_pnts,0], 'o', markersize=2, alpha=.5, color='red')\n",
    "\n",
    "ax.set_title(''.join([i + ', ' for i in event_types]), fontsize=14)\n",
    "ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### pooling loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(align_pnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sides = (95, 230)\n",
    "event_types = ['nose-to-right', ('nose-to-ag', 'nose-to-nose') ] \n",
    "align_pad = 200\n",
    "phase_dict = {'preference': (6500, 6500+11500)}\n",
    "n_iter = 5\n",
    "\n",
    "\n",
    "for the_cond in stat_dict.keys(): # each genotype\n",
    "    print('\\n',the_cond)\n",
    "    if 'linear' in sorted(stat_dict[the_cond].keys())[0]:\n",
    "        print('** Repeated measures detected **')\n",
    "        for the_exp in stat_dict[the_cond].keys(): # each experiment condition\n",
    "            print(the_exp)\n",
    "            \n",
    "            # This needs to be populated with per-experiment version of the following:\n",
    "            \n",
    "                        \n",
    "    else: # same as above loop but without repeated measures for each subject:\n",
    "        print('** No repeated measures detected **')\n",
    "        for the_subj in stat_dict[the_cond]: # each subject\n",
    "                print('\\t',the_subj)\n",
    "                for the_phase in sorted(phase_dict.keys()):\n",
    "                    frames = (phase_dict[the_phase][0], phase_dict[the_phase][1])\n",
    "                    for event_type in event_types:\n",
    "                        if isinstance(event_type, str):\n",
    "                            event_type = [event_type]\n",
    "                        print('\\t', event_type)\n",
    "                        # load traces and labels, extract samples in the phase of interest\n",
    "                        trace_mtx = stat_dict[the_cond][the_subj]['trace_mtx'][frames[0]:frames[1], :]\n",
    "                        df_labels = stat_dict[the_cond][the_subj]['labels']\n",
    "                        align_pnts = df_labels.frame.loc[df_labels.label.isin(event_type)].values - frames[0]\n",
    "                        #print('\\talign points: {}'.format(align_pnts[the_side][event_type]))\n",
    "\n",
    "                        # convert alignment points to raster:\n",
    "                        align_raster = np.zeros((trace_mtx.shape[0],))\n",
    "                        padded_align_pnts = align_pnts\n",
    "                        #padded_align_pnts = np.asarray(\n",
    "                        #    [np.arange(i, i+align_pad) for i in align_pnts[the_side][event_type] if (i+align_pad) < trace_mtx.shape[0]]\n",
    "                        #).flatten()\n",
    "\n",
    "                        if len(align_pnts):\n",
    "                            align_raster[padded_align_pnts] = 1\n",
    "\n",
    "                            print('\\t', trace_mtx.shape, align_raster.shape, min(np.argwhere(align_raster==1)), max(np.argwhere(align_raster==1)))\n",
    "\n",
    "                            # find cells with significant auROC based on time-shuffled distribution:\n",
    "                            auroc = []\n",
    "                            auroc_p = []\n",
    "\n",
    "                            for cellnum,the_trace in enumerate(trace_mtx.transpose()):\n",
    "                                the_auroc = metrics.roc_auc_score(align_raster, the_trace)\n",
    "                                #the_auroc = metrics.average_precision_score(align_raster[frames[0]:frames[1]], the_trace)\n",
    "                                auroc.append(the_auroc)\n",
    "                                shift_auroc = []\n",
    "                                for the_iter in range(n_iter): # time-shuffle traces for null distribution:\n",
    "                                    shift_trace = np.roll(the_trace, np.random.choice(np.arange(len(the_trace))))\n",
    "                                    shift_auroc.append(metrics.roc_auc_score(align_raster, shift_trace) )\n",
    "\n",
    "                                    # precision-recall curve instead of ROC:\n",
    "                                    #shift_auroc.append(metrics.average_precision_score(align_raster[frames[0]:frames[1]], shift_trace) )\n",
    "\n",
    "                                auroc_p.append(prob_from_dist(the_auroc, shift_auroc, hist_range = (0,1), tails = 2))\n",
    "                        else:\n",
    "                            print('\\t** no events found')\n",
    "                            auroc = []\n",
    "                            auroc_p = []\n",
    "                        statstr = 'auroc_' + ''.join([i + ', ' for i in event_type])\n",
    "                        stat_dict[the_cond][the_subj][statstr] = auroc\n",
    "                        statstr = 'auroc_p_' + ''.join([i + ', ' for i in event_type])\n",
    "                        stat_dict[the_cond][the_subj][statstr] = auroc_p\n",
    "                        print('\\n')\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = pickle.load(open('stat_dict.pkl' , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stat_dict.keys():\n",
    "    print(i)\n",
    "    for j in stat_dict[i].keys():\n",
    "        print('\\t',j)\n",
    "        for k in stat_dict[i][j].keys():\n",
    "            print('\\t\\t',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'frame_aligned_data_baclofen_rocAllCotune.pkl'\n",
    "with open(fn, 'wb') as fp:\n",
    "    pickle.dump(stat_dict,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stat_dict['FMR1KO']['OM040']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## no repeated measures pooling / plotting loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = pickle.load(open('frame_aligned_data_20200415_annotated_2.pkl' , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stat_dict.keys():\n",
    "    print(i)\n",
    "    for j in stat_dict[i].keys():\n",
    "        print('\\t',j)\n",
    "        for k in stat_dict[i][j].keys():\n",
    "            print('\\t\\t',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = ['rearing left', 'rearing right', 'grooming']\n",
    "print(event_types), \n",
    "print([''.join([j for j in i]) + '_pos_mod' for i in event_types] + [''.join([j for j in i]) + '_neg_mod' for i in event_types])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = event_types\n",
    "p_thresh = 0.01\n",
    "roc_thresh = .6\n",
    "align_pad = 200\n",
    "frames = (6200,6200+11500)\n",
    "#frames = (10,5500)\n",
    "\n",
    "frac_dict = dict.fromkeys(stat_dict)\n",
    "\n",
    "for the_cond in stat_dict.keys(): # each genotype\n",
    "    print('\\n',the_cond)\n",
    "\n",
    "    frac_dict[the_cond] = dict.fromkeys([''.join([j for j in i]) + '_pos_mod' for i in event_types] + \n",
    "                                        [''.join([j for j in i]) + '_neg_mod' for i in event_types])\n",
    "    \n",
    "    for i in frac_dict[the_cond]:\n",
    "        frac_dict[the_cond][i] = []\n",
    "    print(frac_dict[the_cond].keys())\n",
    "\n",
    "    for the_subj in sorted(stat_dict[the_cond]): # each subject\n",
    "        print('\\t',the_subj)\n",
    "\n",
    "        trace_mtx = stat_dict[the_cond][the_subj]['trace_mtx']\n",
    "        df_labels = stat_dict[the_cond][the_subj]['labels']\n",
    "        \n",
    "        for event_type in event_types:\n",
    "\n",
    "            # create raster from event points:\n",
    "            align_raster = np.zeros((trace_mtx.shape[0],))\n",
    "            if isinstance(event_type, str):\n",
    "                align_pnts = df_labels.frame.loc[df_labels.label.isin([event_type])].values\n",
    "            else:\n",
    "                align_pnts = df_labels.frame.loc[df_labels.label.isin(event_type)].values\n",
    "            padded_align_pnts = align_pnts\n",
    "            align_raster[padded_align_pnts] = 1\n",
    "            \n",
    "            if isinstance(event_type, tuple):\n",
    "                event_type = ''.join([i + ', ' for i in event_type])[:-2]\n",
    "            print('\\t',event_type)\n",
    "            #print('\\talign points: {}'.format(align_pnts[the_side][event_type]))\n",
    "\n",
    "            # load auROC and auROC_p vectors:\n",
    "            stat_str = 'auroc_p_' + event_type\n",
    "            the_auroc_p = stat_dict[the_cond][the_subj][stat_str]\n",
    "            \n",
    "            stat_str = 'auroc_' + event_type\n",
    "            the_auroc = stat_dict[the_cond][the_subj][stat_str]\n",
    "            \n",
    "            pos_mod = np.intersect1d(np.argwhere(np.asarray(the_auroc_p) <= p_thresh).flatten(), np.argwhere(np.asarray(the_auroc) >= roc_thresh).flatten())\n",
    "            neg_mod = np.intersect1d(np.argwhere(np.asarray(the_auroc_p) <= p_thresh).flatten(), np.argwhere(np.asarray(the_auroc) <= (1-roc_thresh)).flatten())\n",
    "\n",
    "            # positively modulated cells:\n",
    "            stat_str = ''.join([j for j in event_type]) + '_pos_mod'\n",
    "            stat_str = stat_str.replace(', ','')\n",
    "            if len(pos_mod) and sum(align_raster[frames[0]:frames[1]]): # check for modulated cells and zone entries\n",
    "                avg_trace = np.mean(trace_mtx[:,pos_mod], axis=1)\n",
    "                auc = metrics.roc_auc_score(align_raster[frames[0]:frames[1]], avg_trace[frames[0]:frames[1]])\n",
    "                #frac_dict[the_cond][the_exp]['positive_mod_left'].append(auc)\n",
    "                frac_dict[the_cond][stat_str].append(len(pos_mod)/len(the_auroc))\n",
    "                if len(pos_mod==1):\n",
    "                    pos_mod= pos_mod[0]\n",
    "                #frac_dict[the_cond][the_exp]['positive_mod_left'].append(np.mean(the_auroc[pos_mod_l]))\n",
    "            else:\n",
    "                #frac_dict[the_cond][the_exp]['positive_mod_left'].append(np.nan)\n",
    "                frac_dict[the_cond][stat_str].append(0)\n",
    "                \n",
    "            # negatively modulated cells:\n",
    "            stat_str = ''.join([j for j in event_type]) + '_neg_mod'\n",
    "            stat_str = stat_str.replace(', ','')\n",
    "            if len(neg_mod) and sum(align_raster[frames[0]:frames[1]]): # check for modulated cells and zone entries\n",
    "                avg_trace = np.mean(trace_mtx[:, neg_mod], axis=1)\n",
    "                auc = metrics.roc_auc_score(align_raster[frames[0]:frames[1]], avg_trace[frames[0]:frames[1]])\n",
    "                #frac_dict[the_cond][the_exp]['positive_mod_left'].append(auc)\n",
    "                frac_dict[the_cond][stat_str].append(len(neg_mod)/len(the_auroc))\n",
    "                if len(neg_mod==1):\n",
    "                    neg_mod = neg_mod[0]\n",
    "                #frac_dict[the_cond][the_exp]['positive_mod_left'].append(np.mean(the_auroc[pos_mod_l]))\n",
    "            else:\n",
    "                #frac_dict[the_cond][the_exp]['positive_mod_left'].append(np.nan)\n",
    "                frac_dict[the_cond][stat_str].append(0)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frac_dict.keys())\n",
    "print(frac_dict['FMR1KO'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pie charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_cmap = plt.get_cmap('Dark2')\n",
    "\n",
    "stats_to_do = ['rearing left_pos_mod', 'rearing left_neg_mod', \n",
    "               'rearing right_pos_mod', 'rearing right_neg_mod',\n",
    "               'grooming_pos_mod', 'grooming_neg_mod'\n",
    "              ]\n",
    "\n",
    "stat_labels = ['rearing left +', 'rearing left -', \n",
    "               'rearing right +', 'rearing right -',\n",
    "               'grooming +', 'grooming -',\n",
    "               'untuned']\n",
    "\n",
    "f,ax = plt.subplots(1, 1, figsize = (6,6))\n",
    "\n",
    "pie_vals = [np.nanmedian(frac_dict['FMR1CTRL'][i]) for i in stats_to_do] + [1-np.sum([np.nanmedian(frac_dict['FMR1CTRL'][i]) for i in stats_to_do])]\n",
    "#print(pie_vals)\n",
    "pie_col = [pie_cmap(i) for i in range(len(pie_vals))]\n",
    "pie_col[-1] = 'lightgray'\n",
    "text_col = pie_col.copy()\n",
    "text_col[-1] = 'black'\n",
    "\n",
    "wedges,texts = ax.pie(pie_vals, radius = 1,colors = pie_col, wedgeprops = dict(width=.4, edgecolor='w'))\n",
    "# pie labels:\n",
    "bbox_props = dict(boxstyle=\"square,pad=0.01\",fc=\"w\", ec=\"w\", lw=1)\n",
    "kw = dict(arrowprops=dict(arrowstyle=\"-\"), bbox=bbox_props, zorder=0, va=\"bottom\")\n",
    "for i, p in enumerate(wedges):\n",
    "    ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
    "    y = np.sin(np.deg2rad(ang))\n",
    "    x = np.cos(np.deg2rad(ang))\n",
    "    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
    "    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle}, color='black')\n",
    "    ax.annotate(str(round(pie_vals[i]*100,1)) + '% '+stat_labels[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n",
    "                horizontalalignment=horizontalalignment, **kw, color=text_col[i])\n",
    "\n",
    "#f.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "f,ax = plt.subplots(1, 1, figsize = (6,6))\n",
    "\n",
    "pie_vals = [np.nanmedian(frac_dict['FMR1KO'][i]) for i in stats_to_do] + [1-np.sum([np.nanmedian(frac_dict['FMR1KO'][i]) for i in stats_to_do])]\n",
    "#print(pie_vals)\n",
    "\n",
    "pie_col = [pie_cmap(i) for i in range(len(pie_vals))]\n",
    "pie_col[-1] = 'lightgray'\n",
    "text_col = pie_col.copy()\n",
    "text_col[-1] = 'black'\n",
    "wedges,texts = ax.pie(pie_vals, radius = 1,colors = pie_col, wedgeprops = dict(width=.4, edgecolor='w'))\n",
    "# pie labels:\n",
    "bbox_props = dict(boxstyle=\"square,pad=0.01\",fc=\"w\", ec=\"w\", lw=1)\n",
    "kw = dict(arrowprops=dict(arrowstyle=\"-\"), bbox=bbox_props, zorder=0, va=\"bottom\")\n",
    "for i, p in enumerate(wedges):\n",
    "    ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
    "    y = np.sin(np.deg2rad(ang))\n",
    "    x = np.cos(np.deg2rad(ang))\n",
    "    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
    "    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle}, color='black')\n",
    "    ax.annotate(str(round(pie_vals[i]*100,1)) + '% '+stat_labels[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n",
    "                horizontalalignment=horizontalalignment, **kw, color=text_col[i])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Pooled Modulation Index (mean auROC, ensemble auROC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = pickle.load(open('frame_aligned_data_20200415_annotated.pkl' , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stat_dict.keys():\n",
    "    print(i)\n",
    "    for j in stat_dict[i].keys():\n",
    "        print('\\t',j)\n",
    "        for k in stat_dict[i][j].keys():\n",
    "            print('\\t\\t',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = ['nose-to-nose', 'nose-to-ag', 'boxing', 'nose-to-right', 'nose-to-left', \n",
    "               'approach left', 'approach right', 'boxing', ('nose-to-left', 'nose-to-nose', 'nose-to-ag')]\n",
    "print(event_types)\n",
    "print([''.join([j for j in i]) + '_pos_mod' for i in event_types] + [''.join([j for j in i]) + '_neg_mod' for i in event_types])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = event_types\n",
    "p_thresh = 0.01\n",
    "roc_thresh = .6\n",
    "align_pad = 200\n",
    "frames = (6200,6200+11500)\n",
    "#frames = (10,5500)\n",
    "\n",
    "frac_dict = dict.fromkeys(stat_dict)\n",
    "\n",
    "for the_cond in stat_dict.keys(): # each genotype\n",
    "    print('\\n',the_cond)\n",
    "\n",
    "    frac_dict[the_cond] = dict.fromkeys([''.join([j for j in i]) + '_pos_mod' for i in event_types] + \n",
    "                                        [''.join([j for j in i]) + '_neg_mod' for i in event_types])\n",
    "    \n",
    "    for i in frac_dict[the_cond]:\n",
    "        frac_dict[the_cond][i] = []\n",
    "    print(frac_dict[the_cond].keys())\n",
    "\n",
    "    for the_subj in sorted(stat_dict[the_cond]): # each subject\n",
    "        print('\\t',the_subj)\n",
    "\n",
    "        trace_mtx = stat_dict[the_cond][the_subj]['trace_mtx']\n",
    "        df_labels = stat_dict[the_cond][the_subj]['labels']\n",
    "        \n",
    "        for event_type in event_types:\n",
    "\n",
    "            # create raster from event points:\n",
    "            align_raster = np.zeros((trace_mtx.shape[0],))\n",
    "            if isinstance(event_type, str):\n",
    "                align_pnts = df_labels.frame.loc[df_labels.label.isin([event_type])].values\n",
    "            else:\n",
    "                align_pnts = df_labels.frame.loc[df_labels.label.isin(event_type)].values\n",
    "            padded_align_pnts = align_pnts\n",
    "            align_raster[padded_align_pnts] = 1\n",
    "            \n",
    "            if isinstance(event_type, tuple):\n",
    "                event_type = ''.join([i + ', ' for i in event_type])[:-2]\n",
    "            print('\\t',event_type)\n",
    "            #print('\\talign points: {}'.format(align_pnts[the_side][event_type]))\n",
    "\n",
    "            # load auROC and auROC_p vectors:\n",
    "            stat_str = 'auroc_p_' + event_type\n",
    "            the_auroc_p = stat_dict[the_cond][the_subj][stat_str]\n",
    "            \n",
    "            stat_str = 'auroc_' + event_type\n",
    "            the_auroc = stat_dict[the_cond][the_subj][stat_str]\n",
    "            \n",
    "            pos_mod = np.intersect1d(np.argwhere(np.asarray(the_auroc_p) <= p_thresh).flatten(), np.argwhere(np.asarray(the_auroc) >= roc_thresh).flatten())\n",
    "            neg_mod = np.intersect1d(np.argwhere(np.asarray(the_auroc_p) <= p_thresh).flatten(), np.argwhere(np.asarray(the_auroc) <= (1-roc_thresh)).flatten())\n",
    "\n",
    "            # positively modulated cells:\n",
    "            stat_str = ''.join([j for j in event_type]) + '_pos_mod'\n",
    "            stat_str = stat_str.replace(', ','')\n",
    "            if len(pos_mod) and sum(align_raster[frames[0]:frames[1]]): # check for modulated cells and zone entries\n",
    "                avg_trace = np.mean(trace_mtx[:,pos_mod], axis=1)\n",
    "                auc = metrics.roc_auc_score(align_raster[frames[0]:frames[1]], avg_trace[frames[0]:frames[1]])\n",
    "                frac_dict[the_cond][stat_str].append(auc)\n",
    "                #frac_dict[the_cond][stat_str].append(len(pos_mod)/len(the_auroc))\n",
    "                #frac_dict[the_cond][stat_str].append(np.mean([the_auroc[i] for i in pos_mod]))\n",
    "                if len(pos_mod==1):\n",
    "                    pos_mod= pos_mod[0]\n",
    "                #frac_dict[the_cond][the_exp]['positive_mod_left'].append(np.mean(the_auroc[pos_mod_l]))\n",
    "            else:\n",
    "                frac_dict[the_cond][stat_str].append(np.nan)\n",
    "                #frac_dict[the_cond][stat_str].append(0)\n",
    "                \n",
    "            # negatively modulated cells:\n",
    "            stat_str = ''.join([j for j in event_type]) + '_neg_mod'\n",
    "            stat_str = stat_str.replace(', ','')\n",
    "            if len(neg_mod) and sum(align_raster[frames[0]:frames[1]]): # check for modulated cells and zone entries\n",
    "                avg_trace = np.mean(trace_mtx[:, neg_mod], axis=1)\n",
    "                auc = metrics.roc_auc_score(align_raster[frames[0]:frames[1]], avg_trace[frames[0]:frames[1]])\n",
    "                frac_dict[the_cond][stat_str].append(auc)\n",
    "                #frac_dict[the_cond][stat_str].append(np.mean([the_auroc[i] for i in neg_mod]))\n",
    "                if len(neg_mod==1):\n",
    "                    neg_mod = neg_mod[0]\n",
    "                #frac_dict[the_cond][the_exp]['positive_mod_left'].append(np.mean(the_auroc[pos_mod_l]))\n",
    "            else:\n",
    "                frac_dict[the_cond][stat_str].append(np.nan)\n",
    "                #frac_dict[the_cond][stat_str].append(0)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in frac_dict.keys():\n",
    "    print(a)\n",
    "    for b in frac_dict[a].keys():\n",
    "        print('\\t',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooled ROC:\n",
    "\n",
    "pos_stats = [('nose-to-left_pos_mod', 'nose-to-right_pos_mod'), \n",
    "             ('approach left_pos_mod', 'approach right_pos_mod'),\n",
    "             ('nose-to-leftnose-to-nosenose-to-ag_pos_mod', 'nose-to-left_pos_mod'),\n",
    "             ('nose-to-nose_pos_mod', 'nose-to-ag_pos_mod'),\n",
    "             ('boxing_pos_mod', 'boxing_pos_mod')\n",
    "            ]\n",
    "\n",
    "neg_stats = [('nose-to-left_neg_mod', 'nose-to-right_neg_mod'), \n",
    "             ('approach left_neg_mod', 'approach right_neg_mod'),\n",
    "             ('nose-to-leftnose-to-nosenose-to-ag_neg_mod', 'nose-to-left_neg_mod'),\n",
    "             ('nose-to-nose_neg_mod', 'nose-to-ag_neg_mod'),\n",
    "             ('boxing_neg_mod', 'boxing_neg_mod')\n",
    "            ]\n",
    "\n",
    "expjog = .5\n",
    "condjog = 1\n",
    "cdict = dict(zip(frac_dict.keys(), [plt.get_cmap('tab10')(i) for i in range(len(frac_dict.keys()))]))\n",
    "\n",
    "f,ax = plt.subplots(1,1,figsize=(20,6))\n",
    "\n",
    "cond_offset = 0\n",
    "text_col = []\n",
    "for pos_stat,neg_stat in zip(pos_stats,neg_stats):\n",
    "    for cond_i,the_cond in enumerate(sorted(frac_dict.keys())):\n",
    "        #print(the_cond)\n",
    "        #print(cond_offset, cond_i)\n",
    "        ax.plot([cond_i+cond_offset, cond_i+cond_offset+expjog], \n",
    "                [frac_dict[the_cond][pos_stat[0]],frac_dict[the_cond][pos_stat[1]] ],\n",
    "                'o-', markersize=5, color=cdict[the_cond], alpha=.5 )\n",
    "        ax.plot([cond_i+cond_offset, cond_i+cond_offset+expjog],\n",
    "                  [np.nanmean(frac_dict[the_cond][pos_stat[0]]),np.nanmean(frac_dict[the_cond][pos_stat[1]]) ],\n",
    "                   'o-', color=cdict[the_cond], markersize=10\n",
    "                  )\n",
    "        ax.plot([cond_i+cond_offset, cond_i+cond_offset+expjog], \n",
    "                [frac_dict[the_cond][neg_stat[0]],frac_dict[the_cond][neg_stat[1]] ],\n",
    "                'o-', markersize=5, color=cdict[the_cond], alpha=.5 )\n",
    "        ax.plot([cond_i+cond_offset, cond_i+cond_offset+expjog],\n",
    "                  [np.nanmean(frac_dict[the_cond][neg_stat[0]]),np.nanmean(frac_dict[the_cond][neg_stat[1]]) ],\n",
    "                   'o-', color=cdict[the_cond], markersize=10\n",
    "                  )\n",
    "        text_col.append(cdict[the_cond])\n",
    "        text_col.append(cdict[the_cond])\n",
    "        \n",
    "    cond_offset+=2\n",
    "    \n",
    "ax.set_ylabel('Ensemble AUC', fontsize=14)\n",
    "ax.set_ylim(0, 1)\n",
    "#ax.set_yticks([0,.1,.2])\n",
    "ax.set_xticks(np.arange(0, len(pos_stats)*2, .5))\n",
    "text = ax.set_xticklabels((['nose-to-social cup', 'nose-to-object cup']*2) + (['approach left', 'approach right']*2)+ \n",
    "                   (['cotuned social sniffs', 'non-social sniffs (social cup)']*2) + (['nose-to-nose', 'nose-to-ag']*2) + (['boxing', 'boxing']*2),\n",
    "                   fontsize=12, rotation=325, ha='left')\n",
    "\n",
    "for t,col in zip(text,text_col):\n",
    "    t.set_color(col)\n",
    "    \n",
    "ax.hlines(0.5, xmin = -.25, xmax = len(pos_stats)*2, color = 'crimson', alpha = .75)\n",
    "ax.grid('on', alpha=.5)\n",
    "twoaxis(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t[0].get_color())\n",
    "text_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phase_dict = {'habituate':(500,5500), 'preference':(6500,6500+11500), 'novelty':(18500,18500+11500)}\n",
    "phase_dict = {'preference':(6500,6500+11500)}\n",
    "behaviors = ( ('approach left', 'approach right'), ('rearing left', 'rearing right'), ('nose-to-left', 'nose-to-right'),\n",
    "            ('nose-to-nose', 'nose-to-ag'), ('boxing', 'grooming')\n",
    "            )\n",
    "plot_jog = .5\n",
    "phase_jog = plot_jog * 1.5\n",
    "cond_jog = phase_jog * len(phase_dict)\n",
    "beh_jog = cond_jog * 3\n",
    "\n",
    "x_ticks = []\n",
    "\n",
    "f,ax = plt.subplots(1,1,figsize=(20,6))\n",
    "\n",
    "for beh_i, the_behaviors in enumerate(behaviors):\n",
    "    #print(the_behaviors)\n",
    "    for cond_i, the_cond in enumerate(stat_dict.keys()):\n",
    "        #print(the_cond)\n",
    "        for phase_i, the_phase in enumerate(phase_dict.keys()):\n",
    "            x1 = (cond_i * cond_jog) + (phase_i * phase_jog) + (beh_i * beh_jog)\n",
    "            #print(x1, x1+plot_jog)\n",
    "            for the_subj in stat_dict[the_cond].keys():\n",
    "                df_labels = stat_dict[the_cond][the_subj]['labels']\n",
    "                ax.plot( (x1, x1 + plot_jog), \n",
    "                        (sum(df_labels.label.loc[df_labels.frame.isin(np.arange(phase_dict[the_phase][0],phase_dict[the_phase][1]))].values == the_behaviors[0])/20, \n",
    "                         sum(df_labels.label.loc[df_labels.frame.isin(np.arange(phase_dict[the_phase][0],phase_dict[the_phase][1]))].values == the_behaviors[1])/20), \n",
    "                         'o-', color = cdict[the_cond], markersize=10, alpha=1)\n",
    "            #x_ticks.append( (x1, x1 + plot_jog) )\n",
    "    \n",
    "    x_ticks.append((cond_i * cond_jog) + (phase_i * phase_jog) + (beh_i * beh_jog) - (plot_jog/4) )\n",
    "\n",
    "\n",
    "twoaxis(ax)\n",
    "ax.set_xticks(np.asarray(x_ticks).flatten())\n",
    "ax.set_xticklabels(behaviors, fontsize=14, rotation=345, horizontalalignment='left' )\n",
    "ax.set_ylabel('time spent (s)', fontsize=14)\n",
    "#ax.set_ylim(0,100)\n",
    "ax.grid(color='gray', alpha=.5, axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### PVD (population vector distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = pickle.load(open('frame_aligned_data_20200415_annotated_2.pkl' , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stat_dict.keys():\n",
    "    print(i)\n",
    "    for j in stat_dict[i].keys():\n",
    "        print('\\t',j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_cond = 'FMR1KO'\n",
    "the_subj = 'OM040'\n",
    "\n",
    "trace_mtx = stat_dict[the_cond][the_subj]['trace_mtx']\n",
    "labels = stat_dict[the_cond][the_subj]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_list = ['nose-to-nose']\n",
    "object_list = ['nose-to-right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_frames = labels.frame.loc[(labels.label.isin(social_list))]\n",
    "object_frames = labels.frame.loc[(labels.label.isin(object_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_trace_mtx = trace_mtx[social_frames,:]\n",
    "object_trace_mtx = trace_mtx[object_frames,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(social_frames.values[518:])\n",
    "#print(np.argwhere(np.diff(social_frames.values, prepend=0) > 1).flatten())\n",
    "onsets_social = np.argwhere(np.diff(social_frames.values, prepend=0) > 1).flatten()\n",
    "offsets_social = np.argwhere(np.diff(social_frames.values, append=social_frames.values[-1]+2) > 1).flatten()\n",
    "\n",
    "onsets_object = np.argwhere(np.diff(object_frames.values, prepend=0) > 1).flatten()\n",
    "offsets_object = np.argwhere(np.diff(object_frames.values, append=object_frames.values[-1]+2) > 1).flatten()\n",
    "\n",
    "print('\\n',onsets_social)\n",
    "print(offsets_social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(social_trace_mtx[:,1])\n",
    "ax.vlines(onsets_social, ymin=-1,ymax=4, alpha=.5, color='green')\n",
    "ax.vlines(offsets_social, ymin=-1,ymax=4, alpha=.5, color='red')\n",
    "\n",
    "twoaxis(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1,figsize=(10,4))\n",
    "\n",
    "#ax.plot(social_frames,'o')\n",
    "ax.plot(onsets_social,'o-')\n",
    "ax.plot(offsets_social,'o-')\n",
    "\n",
    "twoaxis(ax)\n",
    "plt.show()\n",
    "\n",
    "h,hb = np.histogram(offsets_object - onsets_object, bins=20, range = (0,120), )\n",
    "f,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "ax.step(hb[:-1]+(hb[1]-hb[0]), h)\n",
    "\n",
    "twoaxis(ax)\n",
    "plt.show()\n",
    "print(hb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(offsets_social - onsets_social))\n",
    "print(np.mean(offsets_object - onsets_object))\n",
    "print(onsets_social)\n",
    "print(offsets_social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_win = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce population vectors to mean(onsets+bin)\n",
    "social_bins = np.asarray([np.arange(i,i+bin_win) for i in social_frames.values[onsets_social]])\n",
    "object_bins = np.asarray([np.arange(i,i+bin_win) for i in object_frames.values[onsets_object]])\n",
    "\n",
    "#social_trace_mtx = trace_mtx[social_frames,:]\n",
    "#object_trace_mtx = trace_mtx[object_frames,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(social_bins.shape)\n",
    "print(object_bins.shape,'\\n')\n",
    "\n",
    "social_binned_mtx = np.asarray([np.mean(trace_mtx[i,:],axis=0) for i in social_bins] )\n",
    "object_binned_mtx = np.asarray([np.mean(trace_mtx[i,:],axis=0) for i in object_bins] )\n",
    "\n",
    "print(np.shape(social_binned_mtx))\n",
    "print(np.shape(object_binned_mtx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(social_trace_mtx.shape)\n",
    "print(object_trace_mtx.shape)\n",
    "print(np.concatenate((social_trace_mtx, object_trace_mtx), axis=0 ).T.shape)\n",
    "print(trace_mtx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build covariance matrix:\n",
    "\n",
    "#the_cov = np.cov(np.concatenate((social_trace_mtx, object_trace_mtx), axis=0 ).transpose())\n",
    "the_cov = np.cov(trace_mtx[::10,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mahal = DistanceMetric.get_metric('mahalanobis', V = the_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(social_trace_mtx.shape)\n",
    "print(object_trace_mtx.shape)\n",
    "print(the_cov.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(the_cov.shape)\n",
    "cmap = plt.get_cmap('tab10')\n",
    "dist_mtx = mahal.pairwise(np.concatenate((social_binned_mtx, object_binned_mtx), axis=0 ), \n",
    "                         np.concatenate((social_binned_mtx, object_binned_mtx), axis=0 ) )\n",
    "\n",
    "f,ax = plt.subplots(1,1,figsize=(20,6))\n",
    "ax.imshow(mahal.pairwise(np.concatenate((social_binned_mtx, object_binned_mtx), axis=0 ), \n",
    "                         np.concatenate((social_binned_mtx, object_binned_mtx), axis=0 ) ), aspect='equal')\n",
    "\n",
    "#ax.imshow(mahal.pairwise(social_trace_mtx[onsets_social[3]:onsets_social[3]+bin_win, :], \n",
    "#                         np.concatenate((social_trace_mtx[:, :],object_trace_mtx), axis=0 ) ), aspect='auto')\n",
    "\n",
    "#ax.imshow(mahal.pairwise(social_trace_mtx[:100,:], np.concatenate((social_trace_mtx[100:,:],object_trace_mtx), axis=0 ) ), aspect='equal')\n",
    "\n",
    "#ax.set_xlim(0,100)\n",
    "plt.show()\n",
    "\n",
    "f,ax = plt.subplots(1,1,figsize=(20,2))\n",
    "#ax.plot(np.median(mahal.pairwise(social_trace_mtx[onsets_social[3]:offsets_social[3],:], \n",
    "#                              np.concatenate((social_trace_mtx[:,:],object_trace_mtx), axis=0 ) ), axis=0) )\n",
    "ax.plot(dist_mtx[1])\n",
    "ax.vlines(len(social_bins), ymin=0,ymax=max(dist_mtx[1]), color='salmon')\n",
    "twoaxis(ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot summary distances for same vs different events:\n",
    "\n",
    "ends = len(social_bins)\n",
    "\n",
    "f,ax = plt.subplots(figsize=(4,4))\n",
    "ax.plot(np.ones_like( np.median(dist_mtx[:ends,:ends], axis=0)), np.median(dist_mtx[:ends,:ends], axis=0), 'o', markersize=6, color=cmap(0), alpha=.5)\n",
    "ax.scatter(1, np.mean(np.median(dist_mtx[:ends,:ends], axis=0)), marker='_', s=750, linewidth=4, color=cmap(0), alpha=.7)\n",
    "\n",
    "ax.plot(np.ones_like( np.median(dist_mtx[ends:,ends:], axis=0))+.25, np.median(dist_mtx[ends:,ends:], axis=0), 'o', markersize=6, color=cmap(0), alpha=.5)\n",
    "ax.scatter(1.25, np.mean(np.median(dist_mtx[ends:,ends:], axis=0)), marker='_', s=750, linewidth=4, color=cmap(0), alpha=.7)\n",
    "\n",
    "ax.plot(np.ones_like( np.median(dist_mtx[:ends,ends:], axis=0))+.5, np.median(dist_mtx[:ends,ends:], axis=0), 'o', markersize=6, color=cmap(1), alpha=.5)\n",
    "ax.scatter(1.5, np.mean(np.median(dist_mtx[:ends,ends:], axis=0)), marker='_', s=750, linewidth=4, color=cmap(1), alpha=.7)\n",
    "\n",
    "ax.plot(np.ones_like( np.median(dist_mtx[ends:,:ends], axis=0))+.75, np.median(dist_mtx[ends:,:ends], axis=0), 'o', markersize=6, color=cmap(1), alpha=.5)\n",
    "ax.scatter(1.75, np.mean(np.median(dist_mtx[ends:,:ends], axis=0)), marker='_', s=750, linewidth=4, color=cmap(1), alpha=.7)\n",
    "\n",
    "ax.set_xlim(.75,2)\n",
    "twoaxis(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
